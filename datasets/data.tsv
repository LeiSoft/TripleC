A00-1004	2000	into Chinese and then translated back to English by the translation model. The Chinese test corpus was the one used in the TREC5 and TREC6 Chinese track. It contains both Chinese queries and their English translations. Our experiments on these two corpora produced the results shown in Tab. 1. The precision of monolingual IR is given as benchmark. In both E-C and C-E CLIR, the translation model achieved around 40% of monolingual precision. To compare with the dictionary-based approach, we employed a ChineseEnglish dictionary, CEDICT (Denisowski, 1999), and an English-Chinese online dictionary (Anonymous, 1999a) to translate queries. For each word of the source query, all the possible translations given by the dictionary are included in the translated query. The Chinese-English dictionary has about the same performace as the translation model, while the English-Chinese dictionary has lower precision than that of the translation model. We also tried to combine the translations given by the translation model and the dictionary. In both C-E and E-C CUR, significant improvements were achieved (as shown in Tab. 1). The improvements show that the translations given by the translation model and the dictio
A00-1008	2000	as soon as the student gives an unexpected response to a tutor question, the tutor needs to be able to This research was supported by NSF grant number 9720359 to CIRCLE, the Center for Interdisciplinary Research on Constructive Learning Environments at the University of Pittsburgh and Carnegie-Mellon University. plan in order to achieve its goals as well as respond appropriately to the student's statement. Yet classical planning is inappropriate for dialogue generation precisely because it assumes an unchanging world. A more appropriate approach is the &quot;practical reason&quot; approach pioneered by Bratman (1987, 1990). According to Bratman, human beings maintain plans and prefer to follow them, but they are also capable of changing the plans on the fly when needed. Bratman's approach has been introduced into computer science under the name of reactive planning (Georgeff and Ingrand 1989, Wilkins et al. 1995). In this paper we discuss the rationale for the use of reactive planning as well as the use of the hierarchical task network (HTN) style of plan operators. Then we describe APE (the Atlas Planning Engine), a dialogue planner we have implemented to embody the above concepts. We demonstrate the us
A00-1009	2000	fl:+] Figure 7: Lexico-Structural Transfer of English Lexeme MOVE to French More general lexico-structural rules for transfer can also be implemented using our grammar rule formalism. Figure 8 gives an English-French transfer rule applied to a weather domain for the transfer of a verb modified by the adverb ALMOST: It almost rained. Il a failli pleuvoir. TRANSFER-RULE: $X [ class :verb ] ( ATTR ALMOST ) FAILLIR [ class:verb ] ( II $X [ mood:inf ] ) Figure 8: English to French Lexico-Structural Transfer Rule with Verb Modifier ALMOST More details on how the structural divergences described in (Don, 1994) can be accounted for using our formalism can be found in (Nasr et al., 1998). 5 The Rule Processing Before being processed, the rules are first compiled and indexed for optimisation. Each module applies the following processing. The rules are assumed to be ordered from most specific to least specific. The application of the rules to the structures is top-down in a recursive way from the first rule to the last. For the main grammar, before applying a grammar rule to a given node, dictionary lookup is carried out in order to first apply the lexeme- or conceptspecific rules associated with this 
A00-1012	2000	arst's system and correctly identified 98.8% of sentence boundaries. Mikheev (1998) optimised this approach and evaluated it on the same test corpus. An accuracy of 99.2477% was reported, to our knowledge this is the highest quoted result for this test set. These three systems achieve very high results for the punctuation disambiguation task. It would seem, then, that this problem has largely been solved. However, it is not clear that these techniques will be as successful for ASR text. We now go on to describe a system which attempts a task similar to sentence boundary detection of ASR text. Beeferman et al. (1998) produced a system, &quot;CYBERPUNC&quot;, which added intra-sentence punctuation (i.e. commas) to the output of an ASR system. They mention that the comma is the most frequently used punctuation symbol and its correct insertion can make a text far more legible. CYBERPUNC operated by augmenting a standard trigram speech recognition model with information about commas; it accesses only lexical information. CYBERPUNC was tested by separating the trigram model from the ASR system and applying it to 2,317 sentences from the Wall Street Journal. The system achieved a precision of 75.6% and recall of 65.6% co
A00-1014	2000	mains. Most existing systems employ dialogue strategies pre-specified during the design phase of the dialogue manager without taking into account characteristics of actual dialogue interactions. More specifically, mixed initiative systems typically employ rules that specify conditions (generally based on local dialogue context) under which initiative may shift from one agent to the other. Previous research, on the other hand, has shown that changes in initiative strategies in human-human dialogues can be dynamically modeled in terms of characteristics of the user and of the on-going dialogue (Chu-Carroll and Brown, 1998) and that adaptability of initiative strategies in dialogue systems leads to better system performance (Litman and Pan, 1999). However, no previous dialogue system takes into account these dialogue characteristics or allows for initiative-oriented adaptation of dialogue strategies. In this paper, we describe MIMIC, a voice-enabled telephone-based dialogue system that provides movie showtime information, emphasizing its dialogue management aspects. MIMIC improves upon previous systems along two dimensions. First, MIMIC automatically adapts dialogue strategies based on participant roles, charact
A00-1015	2000	, reflection can find its methods and fields. JAVOX uses reflection to (1) map from the JSL-textual representation of an object to the actual instance in the running program; (2) find the appropriate j ava . lang.reflect.Methods for an object/method-name combination; and (3) actually invoke the method, once all of its arguments are known. Reflection is very helpful in examining the application program's structure; however, prior to using reflection, EXECUTER needs access to the objects in the running program. To obtain pointers to the objects, JAVOX uses JOIE, a load-time transformation tool (Cohen et al., 1998). JOIE allows us to modify each application class as it is loaded into the virtual machine. The JAVOX transform adds code to every constructor in the application that registers the new object with Executer. Conceptually, the following line is added to every constructor: Executer.register(this). This modification is done as the class is loaded, the compiled copy — on disk — is not changed. This allows the program to still be run without JAVOX, as a non-speech application. EXECUTER can — once it has the registered objects — use reflection to obtain everything else it needs to perform the actions
A00-1016	2000	ograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI's Flakey robot (Konolige et al., 1993) and NCARAI's InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CornmandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the following. If there is a speech interface, the input speech signal is converted into text. Text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user's intended com
A00-1018	2000	ve only one of several possible parts of speech, hence the need to disambiguate them. We do this with a first-order HMM part-ofspeech tagger (Merialdo [13]). I In the rest of the paper, we will use deceptive cognate very losely often to refer to normative usage of word in general. 129 4. Translation models. Being robust, the alignment program will align a pair of texts regardless of possible omissions in the target text. To detect such omissions of text, a probabilistic bilingual dictionary is called upon. This dictionary was estimated along the line of Brown and al.'s first translation model [2]. It is used to align (coarsely) at the word level. In what follows, we assume the reader to be at least remotely familiar with most of these mechanisms. We will however go into more technical details concerning the transducers considering the central role they play in TransCheck. 2.2 Identifying omissions Grammatical correctors greatly relies on complex grammars to identify &quot;typical&quot; mistakes. We could imagine doing something similar for omission detection trying to construct the meaning of every sentences in a text and then &quot;flag&quot; those where semantic discontinuity were found, not unlike wha
A00-1019	2000	 once are mapped to a special &quot;unknown&quot; word es however, these associations have a lower probability than the good ones. We also found few erratic associations (the first time/ c'etait, some hon. members/!, etc) due to distributional artifacts. It is also interesting to note that the good associations we found are not necessary compositional in nature (we must/il faut, people of canada/les canadiens, of course/evidemment, etc). 3.3 Filtering One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints (Gaussier, 1995; Kupiec, 1993; hua Chen and Chen, 94; Fung, 1995; Evans and Zhai, 1996). It is also possible to focus on non-compositional compounds, a key point in bilingual applications (Su et al., 1994; Melamed, 1997; Lin, 99). Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; Furuse and Iida, 96). In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags. An excerpt of the association probabilities of a unit model trained considering
A00-1020	2000	stics for coreference. Our experiments show that SWIZZLE outperformed COCKTAIL on both English and Romanian test documents. The rest of the paper is organized as follows. Section 2 presents COCKTAIL, a monolingual coreference resolution system used separately on both the English and Romanian texts. Section 3 details the data-driven approach used in SWIZZLE and presents some of its resources. Section 4 reports and discusses the experimental results. Section 5 summarizes the 'The name of COCKTAIL is a pun on CogNIAC because COCKTAIL combines a larger number of heuristics than those reported in (Baldwin, 1997). SWIZZLE, moreover, adds new heuristics, discovered from the bilingual aligned corpus. 142 conclusions. 2 COCKTAIL Currently, some of the best-performing and most robust coreference resolution systems employ knowledge-based techniques. Traditionally, these techniques have combined extensive syntactic, semantic, and discourse knowledge. The acquisition of such knowledge is time-consuming, difficult, and error-prone. Nevertheless, recent results show that knowledge-poor methods perform with amazing accuracy (cf. (Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)). For example, CogNIA
A00-1021	2000	oorhees, 1994) but is expected to achieve much better performance since potentially matching phrases in text are classified in a similar and synergistic way. Our system participated in the official TREC Q&A evaluation. For 200 questions in the evaluation set, we were asked to provide a list of 50-byte and 250-byte extracts from a 2-GB corpus. The results are shown in Section 7. Some techniques used by other participants in the TREC evaluation are paragraph indexing, followed by abductive inference (Harabagiu and Maiorano, 1999) and knowledge-representation combined with information retrieval (Breck et al., 1999). Some earlier systems related to our work are FaqFinder (Kulyukin et al., 1998), MURAX (Kupiec, 1993), which uses an encyclopedia as a knowledge base from which to extract answers, and PROFILE (Radev and McKeown, 1997) which identifies named entities and noun phrases that describe them in text. 2 System description Our system (Figure 1) consists of two pieces: an IR component (GuruQA) that which returns matching texts, and an answer selection compo150 nent (AnSel/Werlect) that extracts and ranks potential answers from these texts. This paper focuses on the process of ranking potential answers
A00-1022	2000	ur domain: Lazy Learning: Lazy Learners are also known as memory-based, instance-based, exemplarbased, case-based, experience-based, or nearest neighbor neighbor algorithms. They store all documents as vectors during the learning phase. In the categorization phase, the new document vector is compared to the stored ones and is categorized to same class as the k-nearest neighbors. The distance is measured by computing e.g. the Euclidean distance between the vectors. By changing the number of neighbors k or the kind of distance measure, the amount of generalization can be controlled. We used IB (Aha, 1992), which is part of the MLC++ library (Kohavi and Sommerfield, 1996). Symbolic Eager Learning: This type of learners constructs a representation for document vectors belonging to a certain class during the learning phase, e.g. decision trees, decision rules or probability weightings. During the categorization phase, the representation is used to assign the appropriate class to a new document vector. Several pruning or specialization heuristics can be used to control the amount of generalization. We used ID3 (Quinlan, 1986), C4.5 (Quinlan, 1992) and C5.0, RIPPER (Cohen, 1995), and the Naive Baye
A00-1024	2000	sspellings, neologisms, and names. Capitalization is his sole means of identifying names. However, capitalization information is not available in closed captions. Hence, his system would be ineffective on the closed caption domain with which we are working. (Granger, 1983) uses expectations generated by scripts to analyze unknown words. The drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described; in this case, naval ship-to-shore messages. Research that is similar in technique to that reported here is (Baluja et al., 1999). Baluja and his colleagues use a decision tree classifier to identify proper names in text. They incorporate three types of features: word level (essentially utilizes case information), dictionary-level (comparable to our ispell feature), and POS information (comparable to our POS tagging). Their highest F-score for name identification is 95.2, slightly higher than our name identifier. However, it is difficult to compare the two sets of results since our tasks are slightly different. The goal of Baluja's research, and all other proper name identification research, is to identify all those wor
A00-1025	2000	 these systems (91 correct); without it, our score of 65 correct places our QA system near the middle of this group of eight. Like the work described here, virtually all of the top-ranked TREC8 systems use a combination of IR, and shallow NLP for their QA systems. IBM's AnSel system (Prager et al., 2000), for example, employs finite-state patterns as its primary shallow NLP component. These are used to recognize a fairly broad set of about 20 named entities. The IR component indexes only text passages associated with these entities. The AT&T QA system (Singhal et al., 2000), the Qanda system (Breck et al., 2000), and the SyncMatcher system (Oard et al., 2000) all employ vector-space methods from IR, named entity identifiers, and a fairly simple question type determiner. In addition, SyncMatcher uses a broad-coverage dependency parser to enforce phrase relationship constraints. Instead of the vector space model, the LASSO system (Moldovan et al., 2000) uses boolean search operators for paragraph retrieval. Recognition of answer hypotheses in their system relies on identifying named entities. Finally, the Cymphony QA system (Srihari and Li, 2000) relies heavily on named entity identification; it also e
A00-1031	2000	ons concerning tagging with Markov models. As two examples, (Rabiner, 1989) and (Charniak et al., 1993) give good overviews of the techniques and equations used for Markov models and part-ofspeech tagging, but they are not very explicit in the details that are needed for their application. We argue that it is not only the choice of the general model that determines the result of the tagger but also the various &quot;small&quot; decisions on alternatives. The aim of this paper is to give a detailed account of the techniques used in TnT. Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993). The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996). For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999). 2 Architecture 2.1 The Underlying Model TnT uses second order Markov models for part-ofspeech tagging. The states of the model represent tags, outputs represent the words. Transition probabilities depend on the states, thus pairs of tags. Output probabilities only depend on the most recent cat
A00-2004	2000	thods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998). The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy. 3 Algorithm Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen, 1994) and a sentence boundary disambiguation algorithm (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997) or EAGLE (Reynar et al., 1997) may be used to convert a plain text document into the acceptable input format. 3.1 Similarity measure Punctuation and uninformative words are removed from ea
A00-2022	2000	 termed 'local ambiguity packing' (Tomita, 1985), and the structure built up by the parser, a 'parse forest' (Billot & Lang, 1989). Context free (CF) grammars represent linguistic objects in terms of atomic category symbols. The test for duplicate parse items—and thus being able to pack the subanalyses associated with them—is equality of category symbols. In the final parse forest every different combination of packed nodes induces a distinct, valid parse tree. Most existing unification-based parsing systems either implicitly or explicitly contain a context-free core. For example, in the CLE (Alshawi, 1992) the (manually-assigned) functors of the Prolog terms forming the categories constitute a CF 'backbone'. In the Alvey Tools system (Carroll, 1993) each distinct set of features is automatically given a unique identifier and this is associated with every category containing those features. The packing technique has been shown to work well in practice in these and similar unification-augmented CF systems: the parser first tests for CF category equality, and then either (a) checks that the existing feature structure subsumes the newly derived one (Moore & Alshawi, 1992), or (b) forms an efficient
A00-2028	2000	 the baseline, and one trained with automatic features from the first two exchanges can perform 14% better than the baseline. 1 Introduction Spoken dialogue systems promise efficient and natural access to a large variety of information sources and services from any phone. Systems that support short utterances to select a particular function (through a statement such as &quot;Say credit card, collect or person-to-person&quot;) are saving companies millions of dollars. Research prototypes exist for applications such as personal email and calendars, travel and restaurant information, and personal banking (Baggia et al., 1998; Walker et al., 1998; Seneff et al., 1995; Sanderman et al., 1998; Chu-Carroll and Carpenter, 1999) inter a/ia. Yet there are still many research challenges: current systems are limited in the interaction they support and brittle in many respects. We show how spoken dialogue systems can learn to support more natural interaction on the basis of their previous experience. One way that current spoken dialogue systems are quite limited is in their strategies for detecting and repairing problems that arise in conversation. If a problem can be detected, the system can either transfer the call to a 
A00-2036	2000	es the following two conditions: 1. A takes as input a nondeterministic FA M = (Q, E, go, F) and produces as output a device Dm that, when given a string w as input, decides whether w E L(M); and 2More precisely, the running time for these algorithms is 0(1 VD13 iwl3 min-0 VT!, lw ID. In cases of practical interest, we always have iwi < 2. there exists a polynomial PA such that every DM runs in an amount of time bounded by PA Owl ). We remark that, given a nondeterministic FA M specified as above, known algorithms allow simulation of M on an input string w in time 0(IMIlwl) (see for instance (Aho et al., 1974, Thm. 9.5) or (Sippu and Soisalon-Soininen, 1988, Thm. 3.38)). In contrast, a quasi-determinizer produces a device that simulates M in an amount of time independent of the size of M itself. A standard example of a quasi-determinizer is the so called power-set construction, used to convert a nondeterministic FA into a language-equivalent deterministic FA (see for instance (Hoperoft and Ullman, 1979, Thm. 2.1) or (Sippu and SoisalonSoininen, 1988, Thm. 3.30)). In fact, there exist constants c and c' such that any deterministic FA can be simulated on input string w in an amount of time bounded b
D08-1001	2008	ntify the various elements a report consists of, such as headings – written in bold on a separate line – introducing sections, subheadings – written in bold followed by a colon – introducing subsections, and enumerations starting with indented numbers followed by a period. Going down further, there are paragraphs divided into sentences. Using these structuring elements, a hierarchic data structure comprising all report elements can be induced. Sections and subsections are typed according to their heading. There exist clear recommendations on structuring medical reports, such as E2184-02 (ASTM International, 2002). However, actual medical reports still vary greatly with regard to their structure. Using the aforementioned standard, we assigned the (sub)headings that actually appeared in the data to the closest type, introducing new types only when absolutely necessary. Finally we arrived at a structure model with three label chains: • Sentence level, with 4 labels: Heading, Subheading, Sentence,Enummarker • Subsection level, with 45 labels: Paragraph, Enumelement, None and 42 subsection types (e.g. VitalSigns, Cardiovascular ...) • Section level, with 23 section types (e.g. ReasonForEncounter, Findings,
D08-1002	2008	sed on Functions We report on the AUCONTRAIRE CD system, which addresses each of the above challenges. First, AUCONTRAIRE identifies “functional phrases” statistically (Section 3). Second, AUCONTRAIRE uses these phrases to automatically create a large corpus of apparent contradictions (Section 4.2). Finally, AUCONTRAIRE sifts through this corpus to find genuine contradictions using knowledge about synonymy, meronymy, argument types, and ambiguity (Section 4.3). Instead of analyzing sentences directly, AUCONTRAIRE relies on the TEXTRUNNER Open Information Extraction system (Banko et al., 2007; Banko and Etzioni, 2008) to map each sentence to one or more tuples that represent the entities in the sentences and the relationships between them (e.g., was born in(Mozart,Salzburg)). Using extracted tuples greatly simplifies the CD task, because numerous syntactic problems (e.g., anaphora, relative clauses) and semantic challenges (e.g., quantification, counterfactuals, temporal qualification) are 1Although we focus on function-based CD in our case study, we believe that our observations apply to other types of CD as well. delegated to TEXTRUNNER or simply ignored. Nevertheless, extracted tuples are a convenient a
D08-1004	2008	se it to help train a machine classifier. How to do this, however, is still being explored. 1.1 Hand-crafted rules An obvious option is to have the annotators directly express their knowledge by hand-crafting rules. This 'This work was supported by National Science Foundation grant No. 0347822 and the JHU WSE/APL Partnership Fund. Special thanks to Christine Piatko for many useful discussions. 31 approach remains “data-driven” if the annotators repeatedly refine their system against a corpus of labeled or unlabeled examples. This achieves high performance in some domains, such as NP chunking (Brill and Ngai, 1999), but requires more analytical skill from the annotators. One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning. 1.2 Feature selection by humans More recent work has focused on statistical classifiers. Training such classifiers faces the “credit assignment problem.” Given a training example x with many features, which features are responsible for its annotated class y? It may take many training examples to distinguish useful vs. irrelevant features.1 To reduce the number of training examples needed, one can ask annotators to examine
D08-1006	2008	sentences. A major difference between the two approaches, however, is that in CE the definition of the sentence's neighborhood must be specified in advance by the modeler. In our work, the 'neighborhood' is determined automatically and dynamically as learning proceeds, according to the capabilities of the classifiers used. The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach. In future work we plan to experiment with richer representations, e.g. including long-range n-grams (Rosenfeld, 1996), class n-grams (Brown et al., 1992), grammatical features (Amaya and Benedy, 2001), etc'. The main computational bottleneck in our approach is the generation of negative samples from the current model. Rejection sampling allowed us to use computationally intensive classifiers as our features by reducing the number of classifications that had to be performed during the sampling process. However, if the boosted model strays too far from the baseline P0, these savings will be negated by the very large sentence rejection probabilities that will ensue. This is likely to be the case when richer representations as suggested above are 
D08-1007	2008	se a classifier to predict the most likely word to fill a position in a sentence (in their experiments: a verb) from a set of candidates (sets of verbs), by inspecting the context of the target token (e.g., the presence or absence of a particular nearby word in the sentence). This approach can therefore learn which specific arguments occur with a particular predicate. In comparison, our features are second-order: we learn what kinds of arguments occur with a predicate by encoding features of the arguments. Recent distributed and latentvariable models also represent words with feature vectors (Bengio et al., 2003; Blitzer et al., 2005). Many of these approaches learn both the feature weights and the feature representation. Vectors must be kept low-dimensional for tractability, while learning and inference on larger scales is impractical. By partitioning our examples by predicate, we can efficiently use high-dimensional, sparse vectors. Our technique of generating negative examples is similar to the approach of Okanohara and Tsujii (2007). They learn a classifier to disambiguate actual sentences from pseudo-negative examples sampled from an N-gram language model. Smith and Eisner (2005) also automatica
D08-1009	2008	orating the Markov network. HOLMES makes some important simplifying assumptions. Specifically, we use simple ground tuples to represent extracted assertions (e.g., contains(kale, calcium)). Syntactic problems (e.g., anaphora, relative clauses) and semantic challenges (e.g., quantification, counterfactuals, temporal qualification) are delegated to the extraction system or simply ignored. This paper focuses on scalability for this subset of the TI task. 1.2 Summary of Experimental Results We tested HOLMES on 183 million distinct ground assertions extracted from the Web by the TEXTRUNNER system (Banko et al., 2007), coupled with 159 thousand ground assertions from WordNet (Miller et al., 1990), and a compact set of handcoded inference rules. Given a total of 55 to 145 seconds, HOLMES was able to produce high-quality inferences that doubled the number of answers to example queries in three disparate domains: geography, business, and nutrition. We also evaluated how the speed of HOLMES scaled with the size of its input corpus. In the general case, logical inference over a Horn theory (needed in order to produce the probabilistic network) is polynomial in the number of ground assertions, and hence in the s
D08-1010	2008	 the decoder hardly distinguish the two rules. Intuitively, information of sub-trees covered by nonterminals as well as contextual information of rules are believed NP NN NP NN NN NN DEG DEG X 1 X 2 X 1 X 2 X 1 X 2 levels X 2 standard of X 1 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89–97, Honolulu, October 2008. c�2008 Association for Computational Linguistics NP NP Figure 2: Training examples for rules in Figure 1 to be helpful for rule selection. Recent research showed that contextual information can help perform word or phrase selection. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating wordsense-disambiguation (WSD) system into a phrasebased (Koehn, 2004) and a hierarchical phrasebased (Chiang, 2005) SMT system, respectively. Similar to WSD, Carpuat and Wu (2007a) used contextual information to solve the ambiguity problem for phrases. They integrated a phrase-sensedisambiguation (PSD) model into a phrase-based SMT system and achieved improvements. In this paper, we propose a novel solution for rule selection for syntax-based SMT. We use the maximum entropy approach to combine rich contextual information around a rule
D08-1016	2008	e each Fm, depends implicitly on W and 0.) As usual for log-linear models, Vθ log Z = � Ep(A)[VθFm(A)1 (10) m Since VθFm(A) only depends on the assignment A’s values for variables that are connected to Fm in the factor graph, its expectation under p(A) depends only on the marginalization of p(A) to those variables jointly. Fortunately, BP provides an estimate of that marginal distribution, namely, its belief about the factor Fm, given W and 0 (§4.2).25 Note that the hard constraints do not depend on 0 at all; so their summands in equation (10) will be 0. We employ stochastic gradient descent (Bottou, 2003), since this does not require us to compute the objective function itself but only to (approximately) estimate its gradient as explained above. Alternatively, given any of the MAP decoding procedures from §6, we could use an error-driven learning method such as the perceptron or MIRA.26 8 Experiments We asked: (1) For projective parsing, where higherorder factors have traditionally been incorporated into slow but exact dynamic programming (DP), what are the comparative speed and quality of the BP approximation? (2) How helpful are such higherorder factors—particularly for non-projective parsin
D08-1022	2008	e right. These extracted rules are called minimal rules, which can be glued together to form composed rules with larger tree fragments (e.g. r1 in Fig. 1) (Galley et al., 2006). Our experiments use composed rules. 3 Forest-based Rule Extraction We now extend tree-based extraction algorithm from the previous section to work with a packed forest representing exponentially many parse trees. 3.1 Packed Forest Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Earley, 1970; Billot and Lang, 1989). For example, consider again the Chinese sentence in Example (1) above, which has (at least) two readings depending on the part-ofspeech of the word yˇu: it can be either a conjunction (CC “and”) as shown in Figure 3, or a preposition (P “with”) as shown in Figure 5, with only PP and VPB swapped from the English word order. 3Admissible set (Wang et al., 2007) is also known as “frontier set” (Galley et al., 2004). For simplicity of presentation, we assume every target word is aligned to at least one source word; see Galley et al. (2006) for handling unaligned target words. IP0,6 NPB0,1 VP1,6 B
D08-1034	2008	 of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation(Boas 2002). With the efforts of many researchers (Carreras and Màrquez 2004, 2005, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004), Xue and Palmer (2005) and Xue (2008). Sun and Jurafsky (2004) did the preliminary w
D08-1036	2008	e., equal to one if its argument is true and zero otherwise), The calculus of variations is used to minimize the KL divergence between the desired posterior distribution and the factorized approximation. It turns out that if the likelihood and conjugate prior belong to exponential families then the optimal Q1 and Q2 do too, and there is an EM-like iterative procedure that finds locally-optimal model parameters (Bishop, 2006). This procedure is especially attractive for HMM inference, since it involves only a minor modification to the M-step of the Forward-Backward algorithm. MacKay (1997) and Beal (2003) describe Variational Bayesian (VB) inference for HMMs. In general, the E-step for VB inference for HMMs is the same as in EM, while the M-step is as follows: �θ(`+1) t�|t = f(E[nt',t] + α)/f(E[nt] + mα) (4) �φ(`+1) w|t = f(E[n0w,t] + α0)/f(E[nt] + m0α0) f(v) = exp(IF(v)) where m0 and m are the number of word types and states respectively, IF is the digamma function and the remaining quantities are as in (2). This means that a single iteration can be performed in O(nm2) time, just as for the EM algorithm. 2.3 MCMC sampling algorithms The goal of Markov Chain Monte Carlo (MCMC) algorithms is to
D08-1039	2008	ncies on the syntactic level by conditioning on the predecessing words and their corresponding parent nodes (Chelba and Jelinek, 2000; Roark, 2001). The latter approach was shown to reduce perplexities and improve the WER in speech recognition systems. One drawback is that the parsing process might slow down the system significantly and the approach is complicated to be integrated directly in the search process. Thus, the effect is often shown offline in reranking experiments using n-best lists. One of the simplest models that can be seen in the context of lexical triggers is the IBM model 1 (Brown et al., 1993) which captures lexical dependencies between source and target words. It can be seen as a lexicon containing correspondents of translations of source and target words in a very broad sense since the pairs are trained on the full sentence level. The model presented in this work is very close to the initial IBM model 1 and can be seen as taking another word into the conditioning part, i.e. the triggering items.1 Furthermore, since the second trigger can come from any part of the sentence, we also have a link to long-range monolingual triggers as presented above. A long-range trigram model is pre
D08-1042	2008	alized to have values in the range of [0, 1] to remove any bias due to different sentence lengths. Lodhi et al. (2002) give a dynamic programming algorithm to compute string subsequence kernels in O(nst) time where s and t are the lengths of the two input strings and n is the maximum length of common subsequences one wants to consider. Rousu and Shawe-Taylor (2005) present an improved algorithm which works faster when the vocabulary size is large. Subsequence kernels have been used with success in NLP for text classification (Lodhi et al., 2002; Cancedda et al., 2003), information extraction (Bunescu and Mooney, 2005b) and semantic parsing (Kate and Mooney, 2006). There are, however, some shortcomings of this word subsequence kernel as a measure of similarity between two sentences. Firstly, since it considers all possible common subsequences, it is not sensitive to whether the subsequence is linguistically meaningful or not. For example, the meaningless subsequences “cat was by” and “a was a” will also be considered common between the two sentences by this kernel. Since these subsequences will be used as implicit features by the kernel-based machine learning algorithm, their presence can only hurt the per
D08-1066	2008	nd Wong (Marcu and Wong, 2002) realize that the problem of extracting phrase pairs should be intertwined with the method of probability estimation. They formulate a joint phrase-based model in which a source-target sentence pair is generated jointly. However, the huge number of possible phrase-alignments prohibits scaling up the estimation by Expectation-Maximization (EM) (Dempster et al., 1977) to large corpora. Birch et al (Birch et al., 2006) provide soft measures for including wordalignments in the estimation process and obtain improved results only on small data sets. Coming up-to-date, (Blunsom et al., 2008) attempt a related estimation problem to (Marcu and Wong, 2002), using the expanded phrase pair set of (Chiang, 2005a), working with an exponential model and concentrating on marginalizing out the latent segmentation variable. Also most up-to-date, (Zhang et al., 2008) report on a multi-stage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator and concentrating the efforts on pruning both the space of phrase pairs and the space of (ITG) analyses. The latter two efforts report improved performance, alb
D08-1113	2008	ss. For the most part, the 2 classes are separated based on whether or not the correct output ends in e. This use of latent classes helped address many errors like wronging / wronge or owed / ow). Such missing or surplus final e’s account for 72.5% of the errors for ngrams and 70.6% of the errors for ngrams+x, but only 34.0% of the errors for ngrams+x+latent. The test oracles are between 99.8% – 99.9%, due to the pruned alignment alphabet. As on the inflection task, the insertion limit does not exclude any gold standard paths. 5 Finite-State Feature Implementation We used the OpenFST library (Allauzen et al., 2007) to implement all finite-state computations, using the expectation semiring (Eisner, 2002) for training. Our model is defined by the WFSA Ue, which is used to score alignment strings in E* (section 2.2). We now sketch how to construct U0 from features. n-gram construction The construction that we currently use is quite simple. All of our current features fire on windows of width < 3. We build a WFSA with the structure of a 3-gram language 1087 model over E*. Each of the JE J' states remembers two previous alignment characters ab of history; for each c E E, it has an outgoing arc that accepts c
D09-1003	2009	 the work of Fürstenau and Lapata (2009) who automatically expand a small training set using an automatic dependency alignment of unlabeled sentences. This method was tested on the FrameNet corpus and improved results when compared to a fully-supervised classifier. We will discuss their method in detail in section 5. 3 Semantic role labeling Fillmore (1968) introduced semantic structures called semantic frames, describing abstract actions or common situations (frames) with common roles and themes (semantic roles). Inspired by this idea different resources were constructed, including FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). An alternative approach to semantic role labeling is the framework developed 1See http://www.cnts.ua.ac.be/conll/ for an overview. by Halliday (1994) and implemented by Mehay et al. (2005). PropBank has thus far received the most attention of the research community, and is used in our work. 3.1 PropBank The goal of the PropBank project is to add semantic information to the syntactic nodes in the English Penn Treebank. The main motivation for this annotation is the preservation of semantic roles across different syntactic realizations. Take for instance the 
D09-1053	2009	earch ranking: Model Interpolation approaches and error-driven learning approaches. In model interpolation approaches, the adaptation data is used to derive a domain-specific model (also called in-domain model), which is then combined with the background model trained on the background data. This appealingly simple concept provides fertile ground for experimentation, depending on the level at which the combination is implemented (Bellegarda, 2004). In error-driven learning approaches, the background model is adjusted so as to minimize the ranking errors the model makes on the adaptation data (Bacchiani et al., 2004; Gao et al. 2006). This is arguably more powerful than model interpolation for two reasons. First, by defining a proper error function, the method can optimize more directly the measure used to assess the final quality of the Web search system, e.g., Normalized Discounted Cumulative Gain (Javelin & Kekalainen, 2000) in this study. Second, in this framework, the model can be adjusted to be as fine-grained as necessary. In this study we developed a set of error-driven learning methods based on a boosting algorithm where, in an incremental manner, not only each feature weight could be 505 Procee
D09-1056	2009	aneously, in a relevant application domain for web search services: Zoominfo.com, Spock.com, 123people.com are examples of sites which perform web people search, although with limited disambiguation capabilities. A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task: 11-17% of the queries were composed of a person name with additional terms and 4% were identified as person names (Spink et al., 2004). According to the data available from 1990 U.S. Census Bureau, only 90,000 different names are shared by 100 million people (Artiles et al., 2005). As the amount of information in the WWW grows, more of these people are mentioned in different web pages. Therefore, a query for a common name in the Web will usually produce a list of results where different people are mentioned. This situation leaves to the user the task of finding the pages relevant to the particular person he is interested in. The user might refine the original query with additional terms, but this risks excluding relevant documents in the process. In some cases, the existence of a predominant person (such as a celebrity or a historical figure) makes it likely to dominat
D09-1067	2009	 in better performance. However, additional information about semantic SPs of verbs has not yielded considerable improvement on verb classification although SPs can be strong indicators of diathesis alternations (McCarthy, 2001) and although fairly precise semantic descriptions, including information about verb se1See section 6 for discussion on previous work. lectional restrictions, can be assigned to the majority of Levin classes, as demonstrated by VerbNet (Kipper-Schuler, 2005). SP acquisition from undisambiguated corpus data is arguably challenging (Brockmann and Lapata, 2003; Erk, 2007; Bergsma et al., 2008). It is especially challenging in the context of verb classification where SP models are needed for specific syntactic slots for which the data may be sparse, and the resulting feature vectors integrating both syntactic and semantic features may be high dimensional. However, we wanted to investigate whether better results could be obtained if the features were optimised for richness, the feature extraction for accuracy, and a clustering method capable of dealing with the resulting high dimensional feature space was employed. 2.1 Feature extraction We adopted a recent SCF acquisition system whi
D09-1087	2009	the performance of the PCFG-LA parser, a single generative parser, on both small and large amounts of labeled training data. Our approach achieves stateof-the-art parsing accuracies for a single parser on both English (91.5%) and Chinese (85.2%). 1 Introduction There is an extensive research literature on building high quality parsers for English (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Petrov et al., 2006), however, models for parsing other languages are less well developed. Take Chinese for example; there have been several attempts to develop accurate parsers for Chinese (Bikel and Chiang, 2000; Levy and Manning, 2003; Petrov and Klein, 2007), but the state-of-the-art performance, around 83% F measure on Penn Chinese Treebank (achieved by the Berkeley parser (Petrov and Klein, 2007)) falls far short of performance on English (∼90-92%). As pointed out in (Levy and Manning, 2003), there are many linguistic differences between Chinese and English, as well as structural differences between their corresponding treebanks, and some of these make it a harder task to parse Chinese. Additionally, the fact that the available treebanked Chinese materials are more Mary Harper' ,2 2Human Language
D09-1143	2009	nt tree or of the path linking two entities of the dependency tree. For the design of automatic relation classifiers, we have investigated the impact of dependency structures to the RE task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provid
D09-1144	2009	ld Standard In the first experiment we checked whether predicates which people associate with the test arguments can be automatically extracted by our procedure. For this aim we compared the gold standard with all automatically extracted argumentpredicate relations5 containing some of the 30 cue words as follows. These relations were ranked according to the relatedness measure described in previous section. In line with (Cimiano and Wenderoth, 2007) we exploited an approach common in information retrieval for estimating the quality of correspondence of a ranked output to a gold standard, see (Baeza-Yates and Ribeiro-Neto, 1999). Given some n automatically extracted relations with the highest ranking we calculated a precisionrecall curve expressing precision and recall of our procedure compared to the gold standard. The precision characterizes the procedure exactness, i.e. how many redundant relations are retrieved. The 4The overall gold standard consists of 33 tuples. 5In order to evaluate the procedure extracting APrelations on the basis of the semantic annotation we compared automatically extracted tuples to the gold standard tuples. For the procedure using the syntactic annotation only the AP-pairs were considere
D09-1160	2009	tion. We can consider feature combinations in LLMs by explicitly introducing a new conjunctive feature fF,,y(x, y) that is activated when a particular set of features F0 ⊆ F to be combined is activated (namely, fF,,y(x, y) = Afi,y∈F, fi,y(x, y)). We then introduce an `1-regularized LLM (`1- LLM), in which the weight vector w is tuned so as to maximize the logarithm of the a posteriori probability of the training data: L L(w) = log p(yi|xi) − Ckwk1. (2) i=1 Hyper-parameter C thereby controls the degree of over-fitting (solution sparseness). Interested readers may refer to the cited literature (Andrew and Gao, 2007) for the optimization procedures. 2.2 Support Vector Machines A support vector machine (SVM) is a binary classifier (Cortes and Vapnik, 1995). Training with samples {hxi,yii}Li=1 where xi ∈ {0,1}n and yi ∈ {±1} yields the following decision function: y(x) = sgn(g(x) + b) g(x) = � yjαjφ(xj)Tφ(x), (3) xj∈SV where b ∈ R, φ : Rn 7→ RH and support vectors xj ∈ SV (subset of training samples), each of which is associated with weight αj ∈ R. We hereafter call g(x) the weight function. Nonlinear mapping function φ is chosen to make the training samples linearly separable in RH space. Kernel function k
D10-1002	2010	roduct of the SM7 ST-Prod grammars has the same performance as the product of the SM7 ST-Reg grammars. This could be due to the fact that the ST-Prod grammars are no more diverse than the ST-Reg grammars, as we will show in Section 5. 4.4 ST-Prod-Mult Training When creating a product model of regular grammars, Petrov (2010) used a different random seed for each model and conjectured that the effectiveness of the product grammars stems from the resulting diversity of the individual grammars. Two ways to systematically introduce bias into individual models are to either modify the feature sets (Baldridge and Osborne, 2008; Smith and Osborne, 2007) or to change the training distributions of the individual models (Breiman, 1996). Petrov (2010) attempted to use the second method to train individual grammars on either disjoint or overlapping subsets of the treebank, but observed a performance drop in individual grammars resulting from training on less data, as well as in the performance of the product model. Rather than reducing the amount of gold training data (or having treebank experts annotate more data to support the diversity), we employ the self-training paradigm to train models using a combination of the s
D10-1038	2010	nding to I18N’). Multiple topics seem to occur naturally in social interactions, whether synchronous (e.g., chats, meetings) or asynchronous (e.g., emails, blogs) conversations. In multi-party chat (Elsner and Charniak, 2008) report an average of 2.75 discussions active at a time. In our email corpus, we found an average of 2.5 topics per thread. Topic segmentation is often considered a prerequisite for other higher-level conversation analysis and applications of the extracted structure are broad, encompassing: summarization (Harabagiu and Lacatusu, 2005), information extraction and ordering (Allan, 2002), information retrieval (Dias et al., 2007), and intelligent user interfaces (Dredze et al., 2008). While extensive research has been conducted in topic segmentation for monologues (e.g., (Malioutov and Barzilay, 2006), (Choi et al., 2001)) and synchronous dialogs (e.g., (Galley et al., 2003), (Hsueh et al., 2006)), none has studied the problem of segmenting asynchronous multi-party conversations (e.g., email). Therefore, there is no reliable annotation scheme, no standard corpus, and no agreedupon metrics available. Also, it is our key hypothesis that, because of its asynchronous nature, and 
D10-1052	2010	nglish translation task. 1 Introduction In many Statistical Machine Translation (SMT) systems, alignment represents an important piece of information, from which translation rules are learnt. However, while translation models have evolved from word-based to syntax-based modeling, the de facto alignment model remains word-based (Brown et al., 1993; Vogel et al., 1996). This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). Recent work, e.g. by Blunsom et al. (2009) and Haghihi et al. (2009) just to name a few, show that alignment models that bear closer resemblance to state-of-theart translation model consistently yields not only a better alignment quality but also an improved translation quality. In this paper, we follow this recent effort to narrow the gap between alignment model and translation model to improve translation quality. More concretely, we focus on the reordering component since we observe that the treatment of reordering remains significantly different when comparing alignment versus translation: the reordering component in state-of-the-
D10-1056	2010	this improves the corpus probability. [clark]: Class-based n-grams with morphology (Clark, 2003). This system uses a similar model to the previous one, and also clusters word types (rather than tokens, as the rest of the systems do). The main differences between the systems are that clark uses a slightly different approximate search procedure, and that he augments the probabilistic model with a prior that prefers clusterings where morphologically similar words are clustered together. The morphology component is implemented as a single-order letter HMM. [cw]: Chinese Whispers graph clustering (Biemann, 2006). Unlike the other systems we consider, this one induces the value of |C |rather than taking it as an input parameter.2 The system uses a graph clustering algorithm called Chinese Whispers that is based on contextual similarity. The algorithm works in two stages. The first clusters the most frequent 10,000 words (target words) based on their context statistics, with contexts formed from the most frequent 150-250 words (feature words) that appear ei1Implementations were obtained from: brown: http://www.cs.berkeley.edu/∼pliang/ software/brown-cluster-1.2.zip (Percy Liang), clark: http://www.cs.r
D10-1074	2010	ng into consideration of conversation structures. For each of these levels, we further explore two ways of capturing long distance relations between language constituents: implicit modeling based on the length of distance and explicit modeling based on actual patterns of relations. Our empirical findings have shown that the augmented representation with conversation structures is important, which achieves the best performance when combined with explicit modeling of long distance relations. 1 Introduction Textual entailment has received increasing attention in recent years (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009). Given a segment from a textual document, the task of textual entailment is to automatically determine whether a given hypothesis can be entailed from the segment. The capability of such kind of inference can benefit many text-based applications such as information extraction and question answering. Textual entailment has mainly focused on inference from written text in monologue. Recent years also observed an increasing amount of conversational data such as conversation scripts of meetings, call center records, cou
D10-1100	2010	kewness. We not only try the structures suggested by Nguyen et al. (2009) but also introduce a new sequence structure on dependency trees. We discuss their structures and kernel method in detail in Section 4. 3 Social Event Annotation Data 3.1 Social Event Annotation There has been much work in the past on annotating entities, relations and events in free text, most notably the ACE effort (Doddington et al., 2004). We leverage this work by annotating social events on the English part of ACE 2005 Multilingual Training Data2 that has already been annotated for entities, relations and events. In Agarwal et al. (2010), we introduce a comprehensive set of social events which are conceptually different from the event annotation that already exists for ACE. Since our annotation task is complex and layered, in Agarwal et al. (2010) we present confusion matrices, Cohen’s Kappa, and F-measure values for each of the decision points that the annotators go through in the process of selecting a type and subtype for an event. Our annotation scheme is reliable, achieving a moderate kappa for relation detection (0.68) and a high kappa for relation classification (0.86). We also achieve a high global agreement of 69.7% 
D10-1101	2010	ceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1035–1045, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics Random Fields (CRF) (Lafferty et al., 2001) have been successfully applied to several IE tasks in the past (Peng and McCallum, 2006). A recurring problem, which arises when working with supervised approaches, concerns the domain portability. In the opinion mining context this question has been prominently investigated with respect to opinion polarity analysis (sentiment analysis) in previous research (Aue and Gamon, 2005; Blitzer et al., 2007). Terms as “unpredictable” can express a positive opinion when uttered about the storyline of a movie but a negative opinion when the handling of a car is described. Hence the effects of training and testing a machine learning algorithm for sentiment analysis on data from different domains have been analyzed in previous research. However to the best of our knowledge, these effects have not been investigated regarding the extraction of opinion targets. The contribution of this paper is a CRF-based approach for opinion targets extraction which tackles the problem of domain
D10-1123	2010	pus and freelyavailable knowledge resources such as Freebase. It first computes multiple typedfunctionality scores, representing functionality of the relation phrase when its arguments are constrained to specific types. It then aggregates these scores to predict the global functionality for the phrase. LEIBNIZ outperforms previous work, increasing area under the precisionrecall curve from 0.61 to 0.88. We utilize LEIBNIZ to generate the first public repository of automatically-identified functional relations. 1 Introduction The paradigm of Open Information Extraction (IE) (Banko et al., 2007; Banko and Etzioni, 2008) has scaled extraction technology to the massive set of relations expressed in Web text. However, additional work is needed to better understand these relations, and to place them in richer semantic structures. A step in that direction is identifying the properties of these relations, e.g., symmetry, transitivity and our focus in this paper – functionality. We refer to this problem as functionality identi�cation. A binary relation is functional if, for a given arg1, there is exactly one unique value for arg2. Examples of functional relations are father, death date, birth city, etc. We define a
D11-1138	2011	ation system. In particular, we use a system of source-side reordering rules which, given a parse of the source sentence, will reorder the sentence into a target-side order (Collins et al., 2005). In our experiments we work with a set of EnglishJapanese reordering rules1 and gold reorderings based on human generated correct reordering of an aligned target sentences. We use a reordering score based on the reordering penalty from the METEOR scoring metric. Though we could have used a further downstream measure like BLEU, METEOR has also been shown to directly correlate with translation quality (Banerjee and Lavie, 2005) and is simpler to measure. reorder-score = 1 − # chunks − 1 # unigrams matched − 1 reorder-cost = 1 − reorder-score All reordering augmented-loss experiments are run with the same treebank data as the baseline (the training portions of PTB, Brown, and QTB). The extrinsic reordering training data consists of 10930 examples of English sentences and their correct Japanese word-order. We evaluate our results on an evaluation set of 6338 examples of similarly created reordering data. The reordering cost, evaluation 1Our rules are similar to those from Xu et al. (2009). trans–PTB + Brown + QTB tran
D12-1027	2012	uage Learning, pages 286–296, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 2 Related Work One relevant line of research is on machine translation between closely related languages, which is arguably simpler than general SMT, and thus can be handled using word-for-word translation, manual language-specific rules that take care of the necessary morphological and syntactic transformations, or character-level translation/transliteration. This has been tried for a number of language pairs including Czech–Slovak (Hajiˇc et al., 2000), Turkish– Crimean Tatar (Altintas and Cicekli, 2002), Irish– Scottish Gaelic (Scannell, 2006), and Bulgarian– Macedonian (Nakov and Tiedemann, 2012). In contrast, we have a different objective – we do not carry out full translation but rather adaptation since our ultimate goal is to translate into a third language X. A special case of this same line of research is the translation between dialects of the same language, e.g., between Cantonese and Mandarin (Zhang, 1998), or between a dialect of a language and a standard version of that language, e.g., between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr et al., 2008; Sawa
D12-1037	2012	candidates. In our experiments, we employ the two incremental training methods, i.e. MBUU and EBUU. Both of the hyperparameters A are tuned on NIST05 and set as 0.018 and 0.06 for MBUU and EBUU, respectively. In the incremental training step, only one CPU is employed. Table 2 depicts that testing each sentence with local training method takes 2.9 seconds, which is comparable to the testing time 2.0 seconds with global training method4. This shows that the local method is efficient. Further, compared to the retrieval, the local training is not the bottleneck. Actually, if we use LSH technique (Andoni and Indyk, 2008) in retrieval process, the local method can be easily scaled to a larger training data. 5.3 Results and Analysis Table 3 shows the main results of our local training methods. The EBUU training method significantly outperforms the MERT baseline, and the improvement even achieves up to 2.0 BLEU points on NIST08. We can also see that EBUU and MBUU are comparable on these three test sets. Both of these two local training methods achieve significant improvements over the MERT baseline, which proves the effectiveness of our local training method over global training method. Although both local metho
D12-1084	2012	e machine translation (MT) community provides in many varieties. Bannard and Callison-Burch (2005) as well as Zhao et al. (2008) take one language as the pivot and match two possible translations in the other languages as paraphrases if they share a common pivot phrase. As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted. The paraphrasing task is also strongly related to cross-document event coreference resolution, which is tackled by similar techniques used by the available paraphrasing systems (Bagga and Baldwin, 1999; Tomadaki and Salway, 2005). Most work in paraphrase acquisition has dealt with sentence-level paraphrases, e.g., (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Dolan et al., 2004; Quirk et al., 2004). Our approach for sentential paraphrase extraction is related to the one introduced by Barzilay and Lee (2003), who also employ multiple sequence alignment (MSA). However, they use MSA at the sentence level rather than at the discourse level. We take some core ideas from our previous work on mining script information (Regneri et al., 2010). In this earlier work, we focused on event structu
D13-1038	2013	eption (Regier, 1996; Mojsilovic, 2005; Mitchell et al., 2011) should be pursued in the future. 399 Minimum Effort Extra Effort Pefect Perception 84.2% 88.1% Imperfect Perception 45.2% 51.5% Table 3: Results of comparing minimum effort and extra effort using hypergraphs 4.3.3 The Role of Extra Effort While REG systems have a tendency to produce minimal descriptions, recent psycholinguistic studies have shown that speakers do not necessarily follow the Grice’s maxim of quantity, and they tend to provide redundant properties in their descriptions (Jordan and Walker, 2000; Belke and Meyer, 2002; Arts et al., 2011). With this in mind, we conducted a very simple evaluation on the role of extra effort. Once a set of descriptors are selected based on the minimum cost, one additional descriptor (with the least cost among the remaining attributes or relations) is added to the referential description. We once again solicited the crowd feedback to this set of expressions generated by extra effort. Each expression again received three votes from the crowd. Table 3 shows the results by comparing minimum effort with extra effort when using hypergraphs to generate REs. As indicated here, extra effort (by adding on
D13-1115	2013	tions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the “meaning of words is entirely given by other words” (Bruni et al., 2012b). In this paper, we explore various ways to integrate new perceptual information through nove
D14-1076	2014	. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation: to extract a set of salient and nonredundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. Supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain CRF (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous methods include using integer linear programming (ILP) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that int
D14-1083	2014	fitably exploited for reason classification. Experiments on our reason-annotated corpus of ideological debate posts from four domains demonstrate that sophisticated models of stances and reasons can indeed yield more accurate reason and stance classification results than their simpler counterparts. 1 Introduction In recent years, researchers have begun exploring new opinion mining tasks. One such task is debate stance classi�cation (SC): given a post written for a two-sided topic discussed in an online debate forum, determine which of the two sides (i.e., for or against) its author is taking (Agrawal et al., 2003; Thomas et al., 2006; Bansal et al., 2008; Somasundaran and Wiebe, 2009; Burfoot et al., 2011; Hasan and Ng, 2013b). For example, the author of the post shown in Figure 1 is pro-abortion. Oftentimes, however, it is important to determine not only the author’s stance expressed in her debate posts, but also the reasons why she supports or opposes the issue under debate. Intuitively, given a debate topic such as “Should abortion be banned?” or “Do you support Obamacare?”, it [I feel that abortion should remain legal, or rather, parents should have the power to make the decision themselves and no
D14-1130	2014	lated Work The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes. However, he used undergraduate, non-professional subjects, and did not consider re-tuning. Our experimental design with professional bilingual translators follows our previous work Green et al. (2013a) comparing scratch translation to post-edit. Many research translation UIs have been proposed including TransType (Langlais et al., 2000), Caitra (Koehn, 2009b), Thot (Ortiz-Martínez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b), and CasmaCat (Alabau et al., 2013). However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature. Incremental MT learning has been investigated several times, usually starting from no data (Barrachina et al., 2009; Ortiz-Martínez et al., 2010), via simulated post-editing (Martínez-Gómez et al., 2012; Denkowski et al., 2014a), or via re-ranking (Wäschle et al., 2013). No previous experiments combined large-scale baselines, full re-tuning of the model weights, and HTER optimization. HTER tuning can be simulated by reparameterizing an existing metric.
D14-1157	2014	ith all subsets of TopSh and TopShvar; the best results were obtained using all features in each set. 7 Related Work Studies in sociolinguistics (e.g., (Ng et al., 1993; Ng et al., 1995; Reid and Ng, 2000)) have long established that dialog structure in interactions relates to power and influence. Researchers in the NLP community have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014), online discussion forums (Danescu-Niculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word and phrase patterns, to derivatives of such patterns such as linguistic coordination, to deeper dialogic features such as argumentation and dialog acts. Our work differs from these studies in that we study the correlates of power in topic dynamics. Furthermore, we analyze spoken interactions. 8 Conclusion In this paper, we studied how topic shift patterns in the 2012 Republican presidential debates correlate with the power of candidates. We proposed an alternate formulation of the SIT
D14-1222	2014	 a desert air strip at Edwards Air Force Base, Calif., ending a five-day mission that dispatched the Jupiter-bound Galileo space probe. The 1Examples are from OntoNotes (Weischedel et al., 2011). Bridging anaphora are typed in boldface; antecedents in italics. five astronauts returned to Earth about three hours early because high winds had been predicted at the landing site. Fog shrouded the base before touchdown. Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975; Prince, 1981; Gundel et al., 1993; L¨obner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging r
D15-1148	2015	classification model using a rich set of features that effectively identify these sentences. The findings in this paper point out a definite issue in different languages currently underinvestigated in text-to-text generation systems. One possible way to improve MT systems is to incorporate sentence simplification before translation (Mishra et al., 2014). Future work could use our proposed model to detect heavy sentences that needs such pre-processing. Our findings can also inspire informative features for sentence quality estimation, in which the task is to predict the sentence-level fluency (Beck et al., 2014). We have shown that heavy Chinese sentences are likely to lead to hard to read, disfluent sentences in English. Another important future direction lies in text simplification. In our inspection of parallel Wikipedia/Simple Wikipedia data (Kauchak, 2013), around 23.6% of the aligned sentences involve a single sentence on one side and multiple sentences on another. A similar analysis using ideas from this work can be useful in identifying sentences that needs simplification in the first place. Acknowledgements We would like to express our gratitude to Bonnie Webber for her detailed comments on 
E03-1002	2003	eters of the function g, which records a subset of the information from a set of previous history representations in a new history representation. The training process auto3As is standard, g is the sigmoid activation function applied to a weighted sum of its inputs. Multi-layered neural networks of this form can approximate arbitrary mappings from inputs to outputs (Hornik et al., 1989), whereas a loglinear model alone can only estimate probabilities where the category-conditioned probability distributions P(xidi) of the pre-defined inputs x are in a restricted form of the exponential family (Bishop, 1995). 4We use the cross-entropy error function, which ensures that the minimum of the error function converges to the desired probabilities as the amount of training data increases (Bishop, 1995). This implies that the minimum for any given dataset is an estimate of the true probabilities. We use the on-line version of Backpropagation to perform the gradient descent. 133 matically chooses these parameters based on what information needs to be recorded. The recorded information may be needed to compute the output for the current step, or it may need to be passed on to future history representations
E03-1004	2003	n 6, an example illustrating the generation steps is presented in Sec83 tion 7. For the evaluation of the results we use the BLEU score (Papineni et al., 2001). Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations. We also compare the results with the output generated by the statistical translation system GIZA++/ISI ReWrite Decoder (AlOnaizan et al., 1999; Och and Ney, 2000; Germann et al., 2001), trained on the same parallel corpus. 2 Data Resources 2.1 The Prague Dependency Treebank The Prague Dependency Treebank project (Bohmova et al., 2001) aims at complex annotation of a corpus containing about 1.8M word occurrences (about 80,000 running text sentences) in Czech. The annotation, which is based on dependency syntax, is carried out in three steps: morphological, analytical, and tectogrammatical. The first two have been finished so far, presently, there are about 18,000 sentences tectogrammatically annotated. See Haji 6 et al. (2001) and Haji6ova et al. (2000) for details on analytical and on tectogrammatical annotation, respectively. 2.2 English to Czech translation of Penn Treebank So far, there was no considerably large manuall
E03-1005	2003	osed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments, without imposing any constraints on the size of these fragments. Fragments include, for instance, subtrees of depth 1 (corresponding to context-free rules) as well as entire trees. To appreciate these innovations, it should be noted that the model was radically different from all other statistical parsing models at the time. Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g. in Fujisaki et al. 1989; Black et al. 1992, 1993; Briscoe and I Thanks to Ivan Sag for this pun. Waegner 1992; Pereira and Schabes 1992). The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar. This approach has now gained wide usage, as exemplified by the work of Collins (1996, 1999), Charniak (1996, 1997), Johnson (1998), Chiang (2000), and many others. The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset. This innovation has not b
E03-1007	2003	 Question Inversion you did say the eighteenth? Verb Treatment you_did say the eighteenth? Catalan Sentence has dit el divuit ? Spanish Sentence i, has dicho el dieciocho ? ing operation. This makes it impossible to translate the verb itself, because it is then unknown to the system. The same holds for combinations of pronouns and verbs that are unseen in training, e. g. the training corpus contains the bigram 'I went', but not the one 'she went'. In order to overcome this problem, we train our lexicon model using maximum entropy. 5.1 The Maximum Entropy Approach The maximum entropy approach (Berger et al., 1996) presents a powerful framework for the combination of several knowledge sources. This principle recommends to choose the distribution which preserves as much uncertainty as possible in terms of maximizing the entropy. The distribution is required to satisfy constraints, which represent facts known from the data. These constraints are expressed on the basis of feature functions hu,(s,t), where (s, t) is a pair of source and target word. The lexicon probability of a source word given the target word has the following functional form 1 t) Z(t) exp [Y‘ with the normalization factor Z(t) = Eexp [E 
E09-1100	2009	asks since the first Bakeoff (Bakeoff-1, or Bakeoff-2003)3 (Sproat and Emerson, 2003). Up to Bakeoff-4, seven word segmentation standards have been proposed. However, this does not effectively solve the open problem what a Chinese word should exactly be but raises another issue: what a segmentation standard should be selected for the successive application. As word often plays a basic role for the further language processing, if it cannot be determined in a unified way, then all successive tasks will be affected more or less. Motivated by dependency representation for syntactic parsing since (Collins, 1999) that has been drawn more and more interests in recent years, we suggest that character-level dependencies can be adopted to alleviate this difficulty in Chinese processing. If we regard traditional word boundary as a linear representation for neighbored characters, then character-level dependencies can provide a way to represent non-linear relations between non-neighbored characters. To show that character dependencies can be useful, we develop a parsing scheme for the related learning task and demonstrate its effectiveness. The rest of the paper is organized as follows. The next section show
E12-1068	2012	which is a useful approach for dealing with issues of word-formation. However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features. So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing. Much previous work looks at the impact of using source side information (i.e., feature functions on the aligned English), such as those of Avramidis and Koehn (2008), Yeniterzi and Oflazer (2010) and others. Toutanova et. al.’s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information. Using additional source side information beyond the markup did not produce a gain in performance. For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources (e.g., POS-tags Sty
E14-1023	2014	We also introduce novel semantic tree kernels that help us improve the performance of the best reported system on social event detection and classification by a statistically significant margin. We show results for combining the models for the two aforementioned subtasks into the overall task of social network extraction. We show that a combination of features from all three levels of abstractions (lexical, syntactic and semantic) are required to achieve the best performing system. 1 Introduction Social network extraction from text has recently been gaining a considerable amount of attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). One of the reason for this attention, we believe, is that being able to extract social networks from unstructured text may provide a powerful new tool for historians, political scientists, scholars of literature, and journalists to analyze large collections of texts around entities and their interactions. The tool would allow researchers to quickly extract networks and assess their size, nature, and cohesiveness, a task that would otherwise be impossible with corpora numbering millions of documents. It would 
E99-1022	1999	, 1997b) which uses an advanced search function, an advanced selection function and incorporates a coroutining mechanism which supports delayed interpretation. The proposed parser is related to the so-called Lemma Table deduction system (Johnson and D6rre, 1995) which allows the user to specify whether top-down sub-computations are to be tabled. In contrast to Johnson and D6rre's deduction system, though, the selective magic parsing approach combines top-down and bottom-up control strategies. As such it resembles the parser of the grammar development system Attribute Language Engine (ALE) of (Carpenter and Penn, 1994). Unlike the ALE parser, though, the selective magic parser does not presuppose a phrase structure backbone and is more flexible as to which sub-computations are tabled/filtered. 2 Bottom-up Interpretation of Magic-compiled Typed Feature Grammars We describe typed feature grammars and discuss their use in implementing HPSG grammars. Subsequently we present magic compilation of typed 165 Proceedings of EACL '99 feature grammars on the basis of an example and introduce a dynamic bottom-up interpreter that can be used for goal-directed interpretation of magic-compiled typed feature grammars. 2.1 
J00-1003	2000	erministic automaton is determinized, we obtain a third source of exponential behavior. The time and space complexity of the method are thereby bounded by a triple exponential function in the size of the grammar. This theoretical analysis seems to be in keeping with the high costs of applying this method in practice, as will be shown later in this article. As proposed by Pereira and Wright (1997), our implementation applies the approximation separately for each nonterminal occurring in a set N, that reveals selfembedding. A different superset approximation based on LR automata was proposed by Baker (1981) and rediscovered by Heckert (1994). Each individual stack symbol is now translated to one state of the nondeterministic finite automaton. It can be argued theoretically that this approximation differs from the unparameterized RTN approximation from Section 4.1 only under certain conditions that are not likely to occur very often in practice. This consideration is confirmed by our experiments to be discussed later. Our implementation differs from the original algorithm in that the approximation is applied separately for each nonterminal in a set Ni that reveals self-embedding. A generalization
J00-1004	2000	uted from the training counts by simple maximum likelihood estimation. We have applied this method of training statistical dependency transduction models in experiments on English-to-Spanish and English-to-Japanese translations of transcribed spoken utterances. The results of these experiments are described in Section 5; our concluding remarks are in Section 6. 2. Head Transducers 2.1 Weighted Finite-State Head Transducers In this section we describe the basic structure and operation of a weighted head transducer. In some respects, this description is simpler than earlier presentations (e.g., Alshawi 1996); for example, here final states are simply a subset of the transducer states whereas in other work we have described the more general case in which final states are specified by a probability distribution. The simplified description is adequate for the purposes of this paper. Formally, a weighted head transducer is a 5-tuple: an alphabet W of input symbols; an alphabet V of output symbols; a finite set Q of states go, • • • , qs; a set of final states F C Q; and a finite set T of state transitions. A transition from state q to state q' has the form (q, q' ,w, v, a, 0, c) where w is a member o
J00-2001	2000	iming that the interactions are sufficiently minor to be ignored (or at least handled on an ad hoc basis) (Reiter 1994). While this certainly has appeal as a design methodology, it seems reckless to assume that problems will never appear. Certainly an approach to generation that does handle these interactions would be an improvement, as long as it didn't require abandoning modularity. There have in fact been attempts to develop modified modular designs that allow generators to handle interactions between the components. These include devices such as interleaving the components (McDonald 1983; Appelt 1983), backtracking on failure (Appelt 1985; Nogier 1989), allowing the linguistic component to interrogate the planner (Mann 1983; Sondheimer and Nebel 1986), and Hovy's notion of restrictive (i.e., bottom-up) planning (Hovy 1988a, 1988c). All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance. The text planning component still has no way to detect and respond to unanticipated interactions on its own initiative.5 4 Danlos still has a separate low-level &quot;syntactic&quot; component, but essentially all of the
J00-2003	2000	 include &quot;voting methods, Bayesian inference, Dempster-Shafer 's method, generalized evidence processing theory, and various ad hoc techniques&quot; (Hall 1992, 135). Clearly, the above characterization is very wide ranging. Consequently, fusion has been applied to a wide variety of pattern recognition and decision theoretic problems— using a plethora of theories, techniques, and tools—including some applications in computational linguistics (e.g., Brill and Wu 1998; van Halteren, Zavrel, and Daelemans 1998) and speech technology (e.g., Bowles and Damper 1989; Romary and Pierre11989). According to Abbott (1999, 290), &quot;While the reasons [that] combining models works so well are not rigorously understood, there is ample evidence that improvements over single models are typical.... A strong case can be made for combining models across algorithm families as a means of providing uncorrelated output estimates.&quot; Our purpose in this paper is to study and exploit such fusion by model (or strategy) combination as a way of achieving performance gains in PbA. 6. Multiple Strategies for PbA We have experimented with five different scoring strategies, used singly and in combination, in an attempt to improve the 
J00-2004	2000	ar, Chapter 8). 224 Melamed Models of Translational Equivalence each i is distinct, and each j is distinct. The label pairs in a given assignment can be generated in any order, so there are 1! ways to generate an assignment of size 1.6 It follows that the probability of generating a pair of bags (B1, B2) with a particular assignment A of size 1 is Pr(Bi, A, B21/, C , trans) = Pr (1) 1! ri E Pr(C)trans(tii, Ari1C). (10) CEC The above equation holds regardless of how we represent concepts. There are many plausible representations, such as pairs of trees from synchronous tree adjoining grammars (Abeille et al. 1990; Shieber 1994; Candito 1998), lexical conceptual structures (Dorr 1992) and WordNet synsets (Fellbaum 1998; Vossen 1998). Of course, for a representation to be used, a method must exist for estimating its distribution in data. A useful representation will reduce the entropy of the trans distribution, which is conditioned on the concept distribution as shown in Equation 10. This topic is beyond the scope of this article, however. I mention it only to show how the models presented here may be used as building blocks for models that are more psycholinguistically sophisticated. To make the transl
J00-2005	2000	minalization decisions are typically made earlier than word-ordering decisions; for example in the three-stage pipelined architecture presented by Reiter and Dale (2000), pronominalization decisions are made in the second stage (microplanning), but word ordering is chosen during the third stage (realization). This means that the microplanner will not be able to make optimal pronominalization decisions in cases where le or la are unambiguous, but l' is not, since it does not know word order and hence whether the pronoun will be abbreviated. Many other such cases are described in Danlos's book (Danlos 1987). The common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints (such as unambiguous reference) or performing linguistic optimizations (such as using pronouns instead of longer referring expressions whenever possible) in cases where the constraints or optimizations depend on decisions made in multiple modules. This is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module. Despite these arguments, most applied NLG systems use a pipelined archit
J00-2009	2000	nd speculates on a few directions for future research. At first I found Chapters 4 through 8 slightly overwhelming, as they introduce several levels of representation, each with its own terminology and acronyms. However, at a second, more-careful, reading, everything falls into place. The resulting approach has at its center a lexicon that partly implements current theories of lexical semantics such as Jackendoff's (1990) and Levin's (1993). The lexicon is used to mediate and map between a language-independent domain model and a language-dependent ontology widely used in NLG, the Upper Model (Bateman 1990). Although the idea of a two-level representation accommodating language-neutral and language-specific requirements is not new (see for example Nirenburg and Levin [1992], Dorr and Voss [1993], and Di Eugenio [1998]), Stede is among the few who make effective use of those two levels in a complex system. Chapter 4 presents the domain model built by means of the description logic system LOOM (MacGregor and Burstein 1991). Stede is specifically interested in verbalizations of situations, to use his own neutral term. Thus, the domain model contains a representation for categories such as states, a
J00-2013	2000	at allow only a certain degree of variability: (1) John makes concessions to his friend. (2) John makes a right turn. (3) *John makes a right turn to his friend. In this example the predicative noun concessions is the real head of the sentence, governing the number and nature of arguments. The light verb makes is in a support role. Roche's paper shows that such constructions, for which &quot;rewriting mechanisms such as context free parsing are at best unnatural,&quot; can be processed by finite-state transducer parsing. Kornai points out that such light-verb constructions were seen in the tradition of Chomsky (1970) as &quot;core cases of transformational grammar.&quot; Roche's article argues persuasively that finite-state models are not just &quot;an efficient but somewhat inaccurate tool,&quot; as might be imagined from a transformational grammar perspective, &quot;but rather one of the best formalisms at hand to represent accurately complex linguistic phenomena.&quot; Kornai suggests also that another &quot;important way in which mainstream syntax is impacted by finite-state techniques can be called 'finite-state to the rescue'. The paper by Schulz and Mikolajewski [Chapter 14, &quot;Between finite state and Prolog: Constraintbased automata
J00-2014	2000	ensky, and McCarthy in 1993. OT reclaims traditional grammar's ability to express surface generalizations (&quot;syllables have onsets,&quot; &quot;no nasal+voiceless obstruent clusters&quot;). Empirically, some surface generalizations are robust within a language, or—perhaps for functionalist reasons— widespread across languages. Derivational theories were forced to posit diverse rules that rescued these robust generalizations from other phonological processes. An OT grammar avoids such &quot;conspiracies&quot; by stating the generalizations directly, as in TwoLevel Morphology (Koskenniemi 1983) or Declarative Phonology (Bird 1995). In OT, the processes that try but fail to disrupt a robust generalization are described not as rules (cf. Paradis 1988), but as lower-ranked generalizations. Such a generalization may fail in contexts where it is overruled by a higher-ranked requirement of the language (or of the underlying form). As Kager emphasizes, this interaction of violable constraints can yield complex surface patterns. OT therefore holds out the promise of simplifying grammars, by factoring all complex phenomena into simple surface-level constraints that partially mask one another.1 Whether this is always possible un
J00-3001	2000	frequency words are extracted. As mentioned in the introduction, the received wisdom is that the windowing method is unreliable for events with a frequency of less than 5. By means of an analysis of the behavior of statistical tests for 2 x 2 contingency tables with sparse data, a method for optimizing the use of these tests has been developed. We hope that this technique will prove to be useful for domains in which the extraction of low-probability events is crucial. Appendix Log-Likelihood Ratio For the general contingency table, table (a) in Table 4, the log-likelihood ratio is defined by (Agresti 1990): G2 = 2 EEnyln( ii where trig = ni±n±iln±±. When we use the specific contingency table for hapax legomena, table (b) in Table 4, we obtain for a specific G2 of X the formula: X/2 = Ain +W C + (W A) ln + C In W + C A) W A' = ln(W - A)w-A - In Ww + ln(W + C)w-Fc - In(W + C - = W-w (W + C - A)w±c-A (W - A)w-A (W + C)w-c Ww (W + C - A)w±c-A • ex/2 314 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words We rewrite the latter equation to: eX/2wW (W — A)W—A eX/2wW(w _ A)A (W — A)&quot; (w + qvv-Pc (W + C — A) w+c—A ' (W + C)&quot; (W + C)c (W + C — A)A (W + C — A)w (W + C — A)c • Because W > A and t
J00-3002	2000	 it assumes the initial presence of the entire sequent to be proved, i.e., it is in principle nonincrementah on the other hand, allowing incrementality on the basis of Cut would reinstate with a vengeance the problem of spurious ambiguity, for then what are to be the Cut formulas? Consequently, the sequent approach is ill-equipped to address the basic asymmetry of language—the asymmetry of its processing in time—and has never been forwarded in a model of the kind of processing phenomena cited in the introduction. 322 Morrill Incremental Processing and Acceptability An alternative formulation (Ades and Steedman 1982, Steedman 1997), which from its inception has emphasized a capacity to produce left-branching, and therefore incrementally processable, analyses, is comprised of combinatory schemata such as the following (together with a Cut rule, feeding one rule application into another): (17) a. A,A\B B BIA,A B b. A = (B I A)\B A B I (A\B) c. A\B,B\C A\C CIB,BIA CIA By a result of Zielonka (1981), the Lambek calculus is not axiomatizable by any finite set of combinatory schemata, so no such combinatory presentation can constitute the logic of concatenation in the sense of Lambek calculus. Combinatory cate
J00-3003	2000	 selectively skip previous DAs in choosing conditioning for DA prediction. Nagata and Morimoto (1993, 1994) may also have been the first to use word ngrams as a miniature grammar for DAs, to be used in improving speech recognition. The idea caught on very quickly: Suhm and Waibel (1994), Mast et al. (1996), Warnke et al. (1997), Reithinger and Klesen (1997), and Taylor et al. (1998) all use variants of backoff, interpolated, or class n-gram language models to estimate DA likelihoods. Any kind of sufficiently powerful, trainable language model could perform this function, of course, and indeed Alexandersson and Reithinger (1997) propose using automatically learned stochastic context-free grammars. Jurafsky, Shriberg, Fox, and Curl (1998) show that the grammar of some DAs, such as appreciations, can be captured by finite-state automata over part-of-speech tags. N-gram models are likelihood models for DAs, i.e., they compute the conditional probabilities of the word sequence given the DA type. Word-based posterior probability estimators are also possible, although less common. Mast et al. (1996) propose the use of semantic classification trees, a kind of decision tree conditioned on word patterns as features. Finally, 
J00-3006	2000	nfinite series of long-enough sentences that parse in time x by this grammar. However, what matters in engineering practice is the average case for a specific grammar. Specific, since a specific grammar belonging to a high complexity class may well prove to parse much faster than the worst grammar of its class, even with the general algorithm, if the possible time-consuming behavior of the algorithm never happens for this grammar. Average, since it can happen that the grammar does admit hard-toparse sentences that are not used (or at least not frequently used) in the real corpus. For example, Radzinsky (1991) proves that Chinese numerals such as wu zhao zhao zhao zhao zhao wu zhao zhao zhao zhao wu zhao zhao zhao wu zhao zhao wu zhao, for the number 5000000000000000005000000000000005000000000005000000005000, are not context-free, which implies that Chinese is not a context-free language and thus might parse in exponential worst-case time. Do such arguments—no doubt important for mathematical linguistics—have any direct consequences for an engineering linguistics? Even if a Chinese grammar includes a non-context-free rule for parsing such numerals, how frequently will it be activated? Does it imply
J00-4001	2000	ken language text genres (i.e., G08-G10) have a lower identification error rate, on average (0.10), than written language text genres (021) as calculated by either multiple regression or discriminant analysis. The classification accuracy of our method related to the text length for the text genre experiment using multiple regression is presented in Figure 4. Due to the stylistic homogeneity of recipes and broadcast news, the accuracy of texts shorter than 500 words (see Table 1) is relatively high. In addition, texts over 1,500 words seem to be classified more reliably. Note that according to Biber (1990, 1993a) a text length of 1,000 words is adequate for representing the distributions of many core linguistic features of a stylistic category. 484 Stamatatos, Fakotakis, and Kokkinakis Text Categorization Table 4 The structure of the Modem Greek weekly newspaper TO BHMA. Section Title (Translation) Description Code A TO BHMA (the tribune) B NEEZ ETIOXEZ (new ages) C TO AAA° BHMA (the other tribune) D ANAIITYEH (development) E H APAX.MH EAE (your money) I El AIKH EKAOZH (special issue) S BIBAIA (books) Z TEXNEE KAI KAAAITEXNEZ (arts and artists) T TA El AIA (travels) Editorials, diaries, report
J00-4002	2000	e to that proposed in Pereira (1990, 1991), with some differences that are commented on below. Like Pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by Lewin. We analyze quantified NPs at the QLF level as illustrated in the QLF for: (19) Every manager uses a computer. ex istsl ( e. pos( pres( u se(e,eve ry( m a na ger), a c.,„„(com put er)))) We assume that every determiner has its own equivalence, which resolves it as a quantifier: sometimes this can be quite a complicated matter, as with any (Alshawi 1990), which will resolve in different ways depending on its linguistic context, but here we avoid this complexity' 6 Separate equivalences might also make it easier to encode determiner-specific preferences, such as that of each for wide scope. A referee points out that the lack of any explicit ordering of application of equivalences makes one natural way of doing this unavailable. But I am not convinced that this would have been the right way in any case. These preferences are just that, not hard and fast rules, so we need to be able to permit all permutations where the context, or the structure,
J01-4001	2001	ng need for the development of robust and inexpensive solutions to meet the demands of practical NLP systems encouraged many researchers to move away from extensive domain and linguistic knowledge and to embark instead upon knowledge-poor anaphora resolution strategies. A number of proposals in the 1990s deliberately limited the extent to which they relied on domain and/or linguistic knowledge and reported promising results in knowledge-poor operational environments (Dagan and Itai 1990, 1991; Lappin and Leass 1994; Nasukawa 1994; Kennedy and Boguraev 1996; Williams, Harvey, and Preston 1996; Baldwin 1997; Mitkov 1996, 1998b). The drive toward knowledge-poor and robust approaches was further motivated by the emergence of cheaper and more reliable corpus-based NLP tools such as partof-speech taggers and shallow parsers, alongside the increasing availability of corpora and other NLP resources (e.g., ontologies). In fact, the availability of corpora, both raw and annotated with coreferential links, provided a strong impetus to anaphora resolu* School of Humanities, Language and Social Sciences, Stafford Street, Wolverhampton WV1 1SB, UK. E-mail: r.mitkov@w1v.ac.uk t 30 Saw Mill River Road, Hawtho
J02-3002	2002	Section 3.2. Here we see a significant impact of the infelicities in the disambiguation of capitalized words and abbreviations on the performance of the SBD rule set. Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8–1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate. The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000). We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus. In row D of Table 4, we summarized our main results
J03-3004	2003	and co-occurrence probabilities. The respective lengths of the putative alignments in terms of characters is also an important factor. Ahrenberg, Andersson, and Merkel (2002) observe that for less widely spoken languages, the relative lack of linguistic tools and resources has forced developers of word alignment tools for such languages to use shallow processing and basic statistical approaches to word linking. Accordingly, they generate lexical correspondences by means of co-occurrence measures and string similarity metrics. More specifically, the notion of the phrasal lexicon (used first by Becker 1975) has been used successfully in a number of areas: • Learnability (Zernik and Dyer 1987) • Text generation (Hovy 1988; Milosavljevic, Tulloch, and Dale 1996) • Speech generation (Rayner and Carter 1997) • Localization (Sch¨aler 1996) More recently, Simard and Langlais (2001) have proposed the exploitation of TMs at a subsentential level, while Carl, Way, and Sch¨aler (2002) and Sch¨aler, Way, and Carl (2003, pages 108–109) describe how phrasal lexicons might come to occupy a central place in a future hybrid integrated translation environment. This, they suggest, may result in a paradigm shift f
J04-3001	2004	benefits problems in which the cost of acquiring raw data is cheap but the cost of annotating them is high, as is certainly the case for many supervised learning tasks in natural language processing. In addition to PP-attachment, as discussed in this article, sample selection has been successfully applied to other classification 271 Computational Linguistics Volume 30, Number 3 applications. Some examples include text categorization (Lewis and Catlett 1994), base noun phrase chunking (Ngai and Yarowsky 2000), part-of-speech tagging (Engelson Dagan 1996), spelling confusion set disambiguation (Banko and Brill 2001), and word sense disambiguation (Fujii et al. 1998). More challenging are learning problems whose objective is not classification, but generation of complex structures. One example in this direction is applying sample selection to semantic parsing (Thompson, Califf, and Mooney 1999), in which sentences are paired with their semantic representation using a deterministic shift-reduce parser. A recent effort that focuses on statistical syntactic parsing is the work by Tang, Lou, and Roukos (2002). Their results suggest that the number of training examples can be further reduced by using a hybrid 
J05-3003	2005	l of 577 frame types and an average of 4.8 frame types per verb. We present a large-scale evaluation of the complete set of forms extracted against the full COMLEX resource. To our knowledge, this is the largest and most complete evaluation of subcategorization frames acquired automatically for English. 1. Introduction In modern syntactic theories (e.g., lexical-functional grammar [LFG] [Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001], head-driven phrase structure grammar [HPSG] [Pollard and Sag 1994], tree-adjoining grammar [TAG] [Joshi 1988], and combinatory categorial grammar [CCG] [Ades and Steedman 1982]), the lexicon is the central repository for much morphological, syntactic, and semantic information. * National Centre for Language Technology, School of Computing, Dublin City University, Glasnevin, Dublin 9, Ireland. E-mail: {rodonovan,mburke,acahill,josef,awayl@computing.dcu.ie. † Centre for Advanced Studies, IBM, Dublin, Ireland. Submission received: 19 March 2004; revised submission received: 18 December 2004; accepted for publication: 2 March 2005. © 2005 Association for Computational Linguistics Computational Linguistics Volume 31, Number 3 Extensive lexical resources, therefore, are 
J05-4005	2005	 larger than a preset threshold. The value of WFA for a given NW 11 candidate ab is defined as: WFA(ab) = 1 if there exists an affix pair (a, x) (or (b, x)) and the string xb (or ax) is a lexical word; 0, otherwise. For example, given an NW 11 candidate -Fn (xia4-gang3, ‘be laid off’), we have WFA(-Fn) = 1 because (�, -F) is an affix pair (they have 32 common stems such as û, icy, ;, F, �, �) and -Ln (shang4-gang3, ‘take over a shift’) is a lexical word. MP (morphological productivity) is a real-valued feature. It is a measure of the productivity of a particular construction, as defined here (Baayen 1989): MP(x) = n1(x) N(x) . (13) MP is strongly related to the Good-Turing estimate. Here, N is the number of tokens of a particular construction found in a corpus, e.g., the number of tokens of all nouns ending in -ì, and n1 is the number of types of that construction, e.g., the number of unique nouns ending in -ì. Intuitively, a higher value of MP indicates a higher probability that (one of) the component parts of a multicharacter string appears to be a word. For example, Sproat and Shih (2002) show that the MP values of Chinese noun affix -ì and verb affix -A are 0.20 and 0.04, respectively, ind
J06-2002	2006	IPiII (the extension of the next property in the description). If computing the intersection of two sets takes constant time then this makes the complexity of interpreting non-vague descriptions linear: O(nd), where nd is the number of properties used. In a vague description, the property last added to the description is context dependent. Worst case, calculating the set corresponding with such a property, of the form size(x) = maxm, for example, involves sorting the distractors as to their size, which may amount to O(n2d) or O(nd log nd) calculations (depending on the sorting algorithm: cf. [Aho et al. 1983] Chapter 8). Once again, the most time-consuming part of the calculation can be performed off-line, since it is the same for all referring expressions. Thus, the worst-case time complexity of interpretation is as follows: The part that can be computed off-line takes O(nd log nd) calculations. The part that has to be computed for each referring expression separately takes O(nd) calculations. Once again, there is a difference with the nongradable case, but the difference is modest, especially regarding the part that cannot be done off-line. One should bear in mind that worst-case theoretical co
J06-2003	2006	h the formula: count(x, E') + α h(x) = (1) count(x, E) + kα where E' is the set of patterns selected for the class, and E is the set of all input data. So, we count how many times x is in the patterns selected for the class versus the total number of occurrences in the training data. Following Collins and Singer (1999), k = 2, because there are two partitions (relevant and irrelevant for the class). α = 0.1 is a smoothing parameter. In order to obtain input data, we replace all the near-synonyms in the text of the dictionary with the term near syn; then we chunk the text with Abney’s chunker (Abney 1996). The training set E is composed of all the verb phrases, noun phrases, adjectival phrases, and adverbial phrases (denoted vx, nx, ax, rx, respectively) that occur more 230 Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences than t times in the text of the dictionary (where t = 3 in our experiments). Phrases that occur very few times are not likely to be significant patterns and eliminating them makes the process faster (fewer iterations are needed). We apply the DL algorithm for each of the classes DENOTATIONAL DISTINCTIONS and ATTITUDE-STYLE DISTINCTIONS. The input to the a
J06-3002	2006	 linear precedence. para(PP) ≈ P(vcopula ≺ PP) (4) Deverbal Nouns. This diagnostic is based on the observation that PPs following a deverbal noun are likely to be arguments, as the noun shares the argument structure of the verb.2 Proper counting of this feature requires identifying a deverbal noun in the head noun position of a noun phrase. We identify deverbal nouns by inspecting their morphology (Quirk et al. 1985). Specifically, the suffixes that can combine 2 Doubts have been cast on the validity of this diagnostic (Sch¨utze 1995), based on work in theoretical linguistics (Grimshaw 1990). Argaman and Pearlmutter (2002), however, have shown that the argument structures of verbs and related nouns are highly correlated. Hence, we keep deverbal noun as a valid diagnostic here, although we show later that it is not very effective. 347 Computational Linguistics Volume 32, Number 3 with verb bases to form deverbal nouns are listed and exemplified in Figure 1 on page 348. This diagnostic can be captured by a probability indicator function, which assigns probability 1 of being an argument to PPs following a deverbal noun and 0 otherwise. � deverb PP — 1 if deverbal n ≺ PP 5 ( ) 0 otherwise ( ) In conclusion, the dia
J07-1005	2007	al Linguistics Volume 33, Number 1 statistical techniques to overcome the brittleness often associated with knowledgebased approaches? We explore these interesting research questions in the domain of medicine, focusing on the information needs of physicians in clinical settings. This domain is well-suited for exploring the posed research questions for several reasons. First, substantial understanding of the domain has already been codified in the Unified Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993). Second, software for utilizing this ontology already exists: MetaMap (Aronson 2001) identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts relations between the concepts. Both systems utilize and propagate semantic information from UMLS knowledge sources: the Metathesaurus, the Semantic Network, and the SPECIALIST lexicon. The 2004 version of the UMLS Metathesaurus (used in this work) contains information about over 1 million biomedical concepts and 5 million concept names from more than 100 controlled vocabularies. The Semantic Network provides a consistent categorization of all concepts represented in the UMLS Metathesaurus. Third, the paradigm
J08-1003	2008	score for all the PARC dependencies. The XLE system achieves an f-score of 74.31%, 6.24 percentage points lower than the score for all the PARC dependencies and a 3.09 percentage point drop against the results obtained by the Bikel-retrained treebank-based LFG resources. The performance of the Bikel retrained– based LFG system suffers less than the XLE system when preds-only dependencies are evaluated. It is important to note that in our f-structure annotation algorithm and treebankbased LFG parsing architectures, we do not claim to provide fully adequate statistical models. It is well known (Abney 1997) that PCFG- or history-based parser approximations to general constraint-based grammars can yield inconsistent probability models 29 We do not include a p-value here as the breakdown by function per sentence was not available to us for the XLE data. 30 The dependency relations we include in preds-only evaluation are: ADJUNCT, AQUANT, COMP, CONJ, COORD FORM, DET FORM, FOCUS INT, MOD, NUMBER, OBJ, OBJ THETA, OBL, OBL AG, OBL COMPAR, POSS, PRON INT, PRON REL, PRT FORM, QUANT, SUBJ, SUBORD FORM, TOPIC REL, XCOMP. 116 Cahill et al. Statistical Parsing Using Automatic Dependency Structures due to lo
J08-2002	2008	the February 2004 data. For the February 2004 data, we used the standard split into training, development, and test sets—the annotations from sections 02–21 formed the training set, section 24 the development, and section 23 the test set. The set of argument labels considered is the set of core argument labels (ARG0 through ARG5) plus the modifier labels (see Figure 1). The training set contained 85,392 propositions, the test set 4,615, and the development set 2,626. We evaluate semantic role labeling models on gold-standard parse trees and parse trees produced by Charniak’s automatic parser (Charniak 2000). For gold-standard parse trees, we preprocess the trees to discard empty constituents and strip functional tags. Using the trace information provided by empty constituents is very useful for improving performance (Palmer, Gildea, and Kingsbury 2005; Pradhan, Ward et al. 2005), but we have not used this information so that we can compare our results to previous work and since automatic systems that recover it are not widely available. 3.2 Evaluation Measures Since 2004, there has been a precise, standard evaluation measure for semantic role labeling, formulated by the organizers of the CoNLL s
J09-1003	2009	l et al. 2001; Cheng 2002; Lapata * Computer Laboratory, William Gates Building, Cambridge CB3 0FD, UK. Nikiforos.Karamanis@cl.cam.ac.uk. ** Department of Computing Science, King’s College, Aberdeen AB24 3UE, UK. t Department of Computer Science, Wivenhoe Park, Colchester CO4 3SQ, UK. t School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. Submission received: 15 May 2006; revised submission received: 15 December 2007; accepted for publication: 7 January 2008. © 2008 Association for Computational Linguistics Computational Linguistics Volume 35, Number 1 2003; Barzilay and Lee 2004; Barzilay and Lapata 2005, among others). Although these approaches often make use of heuristics related to centering, the features of entity coherence they employ are usually defined informally. Additionally, centering-related features are combined with other coherence-inducing factors in ways that are based mainly on intuition, leaving many equally plausible options unexplored. Thus, the answers to the following questions remain unclear: (i) How appropriate is centering for information ordering in text generation? (ii) Which aspects of centering are most useful for this purpose? These are the issues we investigate i
J09-1005	2009	y in active form; rather, we include voice (passive or active) as an important part of the syntactic pattern of an idiomatic combination. 68 Fazly, Cook, and Stevenson Unsupervised Idiom Identification important to note that the nature of the determiner is also affected by other factors, such as the semantic properties of the noun. For this reason, determiner flexibility is sometimes argued not to be a good predictor of the overall syntactic flexibility of an expression. Nonetheless, many researchers consider it as an important part in the process of idiomatization of a verb+noun combination (Akimoto 1999; Kyt¨o 1999; Tanabe 1999). We thus expect a VNIC to mainly appear with one type of determiner. Pluralization. Although the verb constituent of a VNIC is morphologically flexible, the morphological flexibility of the noun relates to its referential status (Grant 2005). Again, one should note that the use of a singular or plural noun in a VNIC may also be affected by the semantic properties of the noun. Recall that during the idiomatization process, the noun constituent may become more abstract in meaning. In this process, the noun may lose some of its nominal features, including number (Akimot
J09-4010	2009	to strongly match a previous request or response e-mail (Doc-Ret), or requests contain terms that are predictive of complete template response e-mails (Doc-Pred). 4 We used a binary representation, rather than a representation based on TF.IDF scores, because important domain-related words, such as monitor and network, are actually quite frequent. Thus, their low TF.IDF score may have an adverse influence on clustering performance. Nonetheless, in the future, it may be worth investigating a TF.IDF-based representation. 5 Significant bigrams are obtained using the n-gram statistics package NSP (Banerjee and Pedersen 2003), which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram (that it is not a collocation). 603 Computational Linguistics Volume 35, Number 4 As discussed in Section 2, there are situations that cannot be addressed by a document-level approach, because requests only predict or match portions of responses. An alternative approach is to look for promising sentences from one or more previous responses, and collate them into a new response. This task can be cast as extractive multi-document summarization. Unlike a document reuse approach, sentence-
J10-3007	2010	formation Science, 3330 Walnut Street, Philadelphia, PA 19104-6389. E-mail: taskar@cis.upenn.edu. Submission received:1 August 2009; revised submission received: 24 December 2009; accepted for publication: 10 March 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 3 MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008). IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation. IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilistically deficient, intractable models that require local heuristic search and are difficult to implement and extend. Many researchers use the GIZA++ software package
J11-1005	2011	 our baseline POS-tagger in Section 4 for automatic POS-tagging. The results of various models 5 http://nlp.cs.nyu.edu/evalb/. 143 Computational Linguistics Volume 37, Number 1 Table 22 Accuracies of various phrase-structure parsers on CTB2 with gold-standard POS-tags. Model LR LP F1 Bikel Thesis 80.9% 84.5% 82.7% Wang 2006 SVM 87.2% 88.3% 87.8% Wang 2006 Stacked 88.3% 88.1% 88.2% Our parser 89.4% 90.1% 89.8% See text for details. evaluated on sentences with less than 40 words and using gold-standard POS-tags are shown in Table 22. The rows represent the model from Bikel and Chiang (2000) and Bikel (2004), the SVM and ensemble models from Wang et al. (2006), and our parser, respectively. The accuracy of our parser is competitive using this test set. The results of various models using automatically assigned POS-tags are shown in Table 23. The rows in the table represent the models from Bikel and Chiang (2000), Levy and Manning (2003), Xiong et al. (2005), Bikel (2004), Chiang and Bikel (2002), the SVM model from Wang et al. (2006), the ensemble system from Wang et al. (2006), and the parser of this article, respectively. Our parser gave comparable accuracies to the SVM and ensemble models from
J12-4003	2012	he predicate being analyzed and thus cannot be generalized to novel predicates. Additional information might be extracted from VerbNet, which groups related verbs together. Features from this resource might generalize better because they apply to entire sets of verbs (and verb-based nouns). Additionally, the model would benefit from a deeper understanding of the relationships that obtain between predicates in close textual proximity. Often, predicates themselves head arguments to other predicates, and, as a result, borrow arguments from those predicates following certain patterns. The work of Blanco and Moldovan (2011) addresses this issue directly with the use of composition rules. These rules would be helpful for implicit argument identification. Lastly, it should be noted that the prediction model described in this article is quite simple. Each candidate is independently classified as filling each missing argument position, and a heuristic post-processing step is performed to arrive at the final labeling. This approach ignores the joint behavior of semantic arguments. We have performed a preliminary investigation of joint implicit argument structures (Gerber, Chai, and Bart 2011); as described in that wo
J13-1008	2013	best model on predicted input. We then modified the ElixirFMbased best model to use the enhanced DET2 feature. This variation yielded a similarly small gain, altogether less than 0.2% from its ElixirFM-free counterparts. 5.2 Functional Gender and Number Features, and the Rationality Feature The ElixirFM lexical resource used previously provided functional NUMBER feature values but no functional GENDER values, nor RAT (rationality, or humanness) values. To address this issue, we use a version of the PATB3 training and dev sets manually annotated with functional gender, number, and rationality (Alkuhlani and Habash 2011).18 This is the first resource providing all three features (ElixirFm only provides functional number, and to some extent functional gender). We conducted experiments with gold features to assess the potential of these features, and with predicted features, obtained from training a simple maximum likelihood estimation classifier on this resource (Alkuhlani and Habash 2012).19 The first part of Table 8 shows that the RAT (rationality) feature is very relevant (in gold), but suffers from low accuracy (no gains in machine-predicted input). The next two parts show the advantages of functional gend
J14-2004	2014	ovide answer justification (Narayanan and Harabagiu 2004). The same idea of mapping event structures was used in a graph-matching approach for enhancing textual entailment (Haghighi, Ng, and Manning 2005). Event coreference information was also used for detecting contradictions in text (de Marneffe, Rafferty, and Manning 2008). Previous NLP approaches for solving event coreference relied on supervised learning methods that explore various linguistic properties in order to decide if a pair of event mentions is coreferential or not (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin 1999; Ahn 2006; Chen and Ji 2009; Chen, Su, and Tan 2010b). In spite of being successful for a particular labeled corpus, in general, these pairwise models are dependent on the domain or language that they are trained on. For instance, in order to adapt a supervised system to run over a collection of documents written in a different language or belonging to a different domain of interest, at least a minimal annotation effort needs to be performed (Daum´e III 2007). Furthermore, because these models are dependent on local pairwise decisions, they are unable to capture a global event distribution at the topic
J15-3005	2015	 hypothesis scores by size, so that hypotheses of different sizes can be fairly compared. We also find that, with this new approach, negative examples can be expanded during training and a single beam applied to the chart, resulting in a conceptually simpler and more effective training algorithm and decoder. 3. CCG-Based Word Ordering 3.1 The CCG Grammar We were motivated to use CCG as one of the grammar formalisms for our syntax-based realization system because of its successful application to a number of related tasks, such as wide-coverage parsing (Hockenmaier 2003; Clark and Curran 2007b; Auli and Lopez 2011), semantic parsing (Zettlemoyer and Collins 2005), wide-coverage semantic analysis (Bos et al. 2004), and generation itself (Espinosa, White, and Mehay 2008). The grammar formalism has been described in detail in those papers, and so here we provide only a short description. CCG (Steedman 2000) is a lexicalized grammar formalism that associates words with lexical categories. Lexical categories are detailed grammatical labels, typically expressing subcategorization information. During CCG parsing, and during our search procedure, categories are combined using CCG’s combinatory rules. For exampl
J86-1002	1986	tions describe our implementation, which was used to investigate the viability of this approach and the performance it can achieve. Computational Linguistics, Volume 12, Number 1, January-March 1986 17 Pamela K. Fink and Alan W. Biermann The Correction of Ill-Formed Input 5 AN IMPLEMENTATION The usefulness of the methodology described above was tested in the implementation of a connected speech understanding system. An off-the-shelf speech recognition device, a Nippon Electric Corporation DP-200, was added to an existing natural language processing system, the Natural Language Computer (NLC) (Ballard 1979, Biermann and Ballard 1980). The expectation system provided the intermediate processing between the errorful output of the speech recognizer and the deep semantics of NLC. The resulting speech understanding system is called the Voice Natural Language Computer with Expectation (VNLCE, Fink 1983). [The current system should be distinguished from an earlier voice system (VNLC, Biermann et al. 1985), which had no expectation and which handled discrete speech where a 300 millisecond pause must follow each word.] It should be emphasized, of course, that the central issue here is the study of expec
J87-3002	1987	ode system by different lexicographers. For example, when codes containing &quot;to be&quot; are elided they mostly occur as illustrated in Figure 4 above. However, sometimes this is represented as [L(to be)1,9]. Presumably this kind of inconsistency arose because one member of the team of lexicographers realised that this form of elision saved more space. This type of error and inconsistency arises because grammatical codes are constructed by hand and no automatic checking procedure is attempted (see Michiels, 1982, for further comment). One approach to this problem is that taken by the ASCOT project (Akkerman et al., 1985; Akkerman, 1986). In this project, a new lexicon is being manually derived from LDOCE. The coding system for the new lexicon is a slightly modified and simplified version of the LDOCE scheme, without any loss of generalisation and expressive power. More importantly, the assignment of codes for problematic or erroneously labelled words is being corrected in an attempt to make the resulting lexicon more appropriate for automated analysis. In the medium term this approach, though time consuming, will be of some utility for producing more reliable lexicons for natural language processing. 208 Com
J90-3003	1990	d we found only 14 percent of the phrases to be discourse-determined. The identification of a preferred phrasing that is independent of discourse also aids us in identifying and characterizing the discourse features that impinge on prosodic phrasing. Several well-known discourse phenomena—coreference, contrast, and parallelism—affected the phrasing of the clause final prepositional phrases in our corpus. We are left with three or four unexplained cases that are suggestive of a discourse explanation. 2.1.3 PHONOLOGICAL LENGTH AND PROSODIC PHRASING The psycholinguistic studies of Martin (1970), Allen (1975), Hillinger et al. (1976), Grosjean et al. (1979), Dommergues and Grosjean (1983), and Gee and Grosjean (1983), responding to the idea of readjusted syntax as the source of prosodic phrasing, show that grammatical structure, even if readjusted, is not in itself a reliable predictor of prosodic phrasing: mismatches between syntax and prosody occur often and systematically, and can be related to specific nonsyntactic factors such as length and word frequency. For example, although prosodic boundaries between subject and verb do occur, there also exist prosodic patterns in which the boundary come
J91-2003	1991	(Halliday and Hasan 1976, p. 329). Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of &quot;unique-name assumption&quot; (Genesereth and Nilsson 1987), &quot;domain closure assumption&quot; (ibid.), &quot;domain circumscription&quot; (cf. Etherington and Mercer 1987), and their kin. Similarly, the notion of R + M-abduction is spiritually related to the &quot;abductive inference&quot; of Reggia (1985), the &quot;diagnosis from first principles&quot; of Reiter (1987), &quot;explainability&quot; of Poole (1988), and the subset principle of Berwick (1986). But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming). 5.2 p-Models The construction of a model of a paragraph, a p-model, must be based on the information contained in the paragraph itself (the object theory) and in the referential level, while the metalevel restricts ways that the model can be constructed, or, in other words, provides criteria for choosing a most pl
J92-1004	1992	n/machine interface using speech require an &quot;understanding&quot; of the intended message. In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation. Current advances in research and development of spoken language systems2 can be found, for example, in the proceedings of the DARPA speech and natural language workshops, as well as in publications from participants of the ESPRIT SUNDIAL project. Representative systems are described in Boisen et al. (1989), De Mattia and Giachin (1989), Niedermair (1989), Niemann (1990), and Young (1989). * Spoken Language Systems Group, Laboratoiy for Computer Science, MIT, Cambridge MA 02139 f This research was supported by DARPA under Contract N00014-89+1332, monitored through the Office of Naval Research. 1 Speech understanding research flourished in the U.S. in the 1970s under DARPA sponsorship. While &quot;understanding&quot; was one of the original goals, none of the systems really placed any emphasis on this aspect of the problem. 2 We will use the term &quot;speech understanding systems&quot; and &quot;spoken language systems&quot;
J97-4003	1997	ate encoding the finite-state automaton of Figure 16 is shown in Figure 18.28 We now have a first complete encoding of the lexical rules and their interaction represented as covariation in lexical entries. The encoding consists of three types of definite clause predicates: 1. Lexical rule predicates representing the lexical rules; 2. Frame predicates specifying the frame for the lexical rule predicates; and 3. Interaction predicates encoding lexical rule interaction for the natural classes of lexical entries in the lexicon. The way these predicates interconnect is represented in Figure 19. 27 Briscoe and Copestake (1996) argue that semi-productivity of lexical rules, which can be understood as a generalization of exceptions to lexical rules, can be integrated with our approach by assigning probabilities to the automaton associated with a particular lexical entry. 28 In order to distinguish the different interaction predicates for the different classes of lexical entries, the compiler indexes the names of the interaction predicates. Since for expository reasons we will only discuss one kind of lexical entry in this paper, we will not show those indices in the examples given. 557 Computational Linguistics Volum
K15-1001	2015	t al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter alia). Feedback in response-based learning is given by a user accepting or rejecting system predictions, but not by user corrections. Lastly, feedback in form of numerical utility values for actions is studied in the frameworks of reinforcement learning (Sutton and Barto, 1998) or in online learning with limited feedback, e.g., multi-armed bandit models (Cesa-Bianchi and Lugosi, 2006). Our framework replaces quantitative feedback with immediate qualitative feedback in form of a structured object that improves upon the utility of the prediction. 3 Coactive
K15-1002	2015	performance for coreference. Our proposed H-Joint-M system achieves the highest performance. Parameters of our proposed system are tuned as a = 0.9, fl = 0.9, �1 = 0.25 and A2 = 0.2. erence model that we implemented, resulting in a traditional pipelined end-to-end coreference system, namely H-M-Coref. We name our new proposed end-to-end coreference resolution system incorporating both the mention head candidate generation module and the joint framework as H-Joint-M. Evaluation Metrics We compare all systems using three popular metrics for coreference resolution: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAFe) (Luo, 2005). We use the average F1 scores (AVG) of these three metrics as the main metric for comparison. We use the v7.0 scorer provided by CoNLL-2012 Shared Task11. We also evaluate the mention detection performance based on precision, recall and F1 score. As mention heads are important for both mention detection and coreference resolution, we also report results evaluated on mention heads. 4.2 Performance for Coreference Resolution Performance of coreference resolution for all systems on the ACE-2004 and CoNLL-2012 datasets is shown in Table 2 and Table 3 res
K15-1003	2015	 tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as Garrette et al. (2014). Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (X\X)/X rather than introducing special conjunction rules. In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Gigaword 5 corpus (Parker et al., 2011) and supplemented Italian with the out-of-domain WaCky corpus (Baroni et al., 1999). For English and Italian, this allowed us to use 100k raw tokens for training (Chinese uses 62k). For Chinese and Italian, for training efficiency, we used only raw sentences that were 50 words or fewer (note that we did not drop tag dictionary set or test set sentences). The English development set was used to tune hyperparameters using grid search, and the same hyperparameters were then used for all three languages. For the category grammar, we used ppunc=0.1, pterm=0.7, pmod=0.2, pfwd=0.5. For the priors, we use αROOT=1, αBIN=100, αUN=100, αTERM=104, αλ=3, αLCTX=αRCTX=103.6 For the context
N01-1001	2001	ying the generation input: How can a client application possibly know about the feature values it has to select in order to drive a surface realizer? The most prominent example of statistical NLG is NITROGEN, a sentence realizer designed for use in general-purpose machine translation (Langkilde and Knight, 1998). Further examples are the generation systems in (Ratnaparkhi, 2000) and (Bangalore and Rambow, 2000) as well as the use of statistical methods for more specific tasks, for example the ordering of NP premodifiers (Shaw and Hatzivassiloglou, 1999). Instance-based Learning methods (IBL) (Aha et al., 1991) differ from statistical methods in that they are lazy learning approaches: they store previously encountered instances in memory and use them directly to process new input, rather than abstracting them into some statistical distribution. IBL is known to be able to learn exceptions in data well and adapt to subregularities. Since this is highly desirable for natural language processing in general, instance-based methods have been used in NLP under various names (example-based, memory-based, case-based; see Daelemans (1999) for an overview). In this paper, we investigate the use of IBLmethods f
N01-1002	2001	 how we annotated and analysed the NP modifiers in a corpus of museum descriptions to discover rules for the selection and realisation of such modifiers, in particular non-referring ones. We implemented the regularities into an extension of the ILEX system to generate complex NPs capable of serving multiple communicative goals. 1 Introduction and Motivation 1.1 Generating Complex NPs In addition to a referring function, noun phrases (NP) can also serve communicative goals such as providing new information about the referent and expressing the speaker's emotional attitude towards the referent (Appelt, 1985; Jordan, 2000). In Example (1) below, the part in italics refers to an object in a museum, and the part in boldface provides additional information about it. (1) This example from the time of the Qianlong Emperor 1736-95 is made of lacquered wood with decoration in gold and red. Such complex NPs appear frequently in human written texts. A natural language generation (NLG) system must be able to produce complex NPs serving multiple goals in order to write texts as humans do. We divide the components of an NP into two parts based on the different functions/communicative goals they serve: a refe
N01-1003	2001	ceptualize its task as consisting of two distinct phases. In the first phase, the sentence-plan-generator 2The meaning of the human ratings and RankBoost scores in Figure 2 are discussed below. (SPG) generates a potentially large sample of possible sentence plans for a given text-plan input. In the second phase, the sentence-plan-ranker (SPR) ranks the sample sentence plans, and then selects the top-ranked output to input to the surface realizer. Our primary contribution is a method for training the SPR. The SPR uses rules automatically learned from training data, using techniques similar to (Collins, 2000; Freund et al., 1998). Our method for training a sentence planner is unique in neither depending on hand-crafted rules, nor on the existence of a text or speech corpus in the domain of the sentence planner obtained from the interaction of a human with a system or another human. We show that the trained SPR learns to select a sentence plan whose rating on average is only 5% worse than the top human-ranked sentence plan. In the remainder of the paper, section 2 describes the sentence planning task in more detail. We then describe the sentence plan generator (SPG) in section 3, the sentence plan
N01-1006	2001	active because they often capture the linguistic features of a corpus in a small and concise set of rules. Transformation-based learning (TBL) (Brill, 1995) is one of the most successful rule-based machine learning algorithms. It is a flexible method which is easily extended to various tasks and domains, and it has been applied to a wide variety of NLP tasks, including part of speech tagging (Brill, 1995), noun phrase chunking (Ramshaw and Marcus, 1999), parsing (Brill, 1996), phrase chunking (Florian et al., 2000), spelling correction (Mangu and Brill, 1997), prepositional phrase attachment (Brill and Resnik, 1994), dialog act tagging (Samuel et al., 1998), segmentation and message understanding (Day et al., 1997). Furthermore, transformationbased learning achieves state-of-the-art performance on several tasks, and is fairly resistant to overtraining (Ramshaw and Marcus, 1994). Despite its attractive features as a machine learning algorithm, TBL does have a serious drawback in its lengthy training time, especially on the larger-sized corpora often used in NLP tasks. For example, a well-implemented transformation-based part-of-speech tagger will typically take over 38 hours to finish training on a 1 mill
